{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2yyVhKZPMw3t",
   "metadata": {
    "id": "2yyVhKZPMw3t"
   },
   "source": [
    "# Assignment 1 at Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5tr3OGxBzOEO",
   "metadata": {
    "id": "5tr3OGxBzOEO"
   },
   "source": [
    "## 1)Import dataset and reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "gMy80vtfQ-z3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMy80vtfQ-z3",
    "outputId": "55bc0d01-3c74-4420-f55e-5653b995b045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test data shape: (50000, 32, 32, 3) (10000, 32, 32, 3)\n",
      "Train/test labels shape: (50000, 1) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import cifar10\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()#MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()#cifar10 dataset\n",
    "\n",
    "print(\"Train/test data shape:\", x_train.shape, x_test.shape)\n",
    "print(\"Train/test labels shape:\", y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# Reshape the data,comment that for CNN model\n",
    "#x_traink = x_train.reshape((-1, 28*28)) #for MNIST dataset\n",
    "#x_testk = x_test.reshape((-1, 28*28)) \n",
    "#x_traink,x_testk,y_traink,y_testk are used for KNN and NCC\n",
    "x_traink = x_train.reshape((-1, 32*32*3)) #for cifar10 dataset\n",
    "x_testk = x_test.reshape((-1, 32*32*3)) \n",
    "\n",
    "# Encode the labels\n",
    "y_traink = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_testk = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "x_train=x_traink#comment this for Cnn model\n",
    "x_test=x_testk#comment this for Cnn model\n",
    "y_train=y_traink\n",
    "y_test=y_testk\n",
    "\n",
    "#we want the transpose matrices for Neural Network from scratch,comment this for keras models\n",
    "x_train=x_train.T\n",
    "x_test=x_test.T\n",
    "y_train=y_train.T\n",
    "y_test=y_test.T\n",
    "\n",
    "#Normalization\n",
    "layertrain = layers.Normalization()\n",
    "layertrain.adapt(x_train)\n",
    "x_train = layertrain(x_train)\n",
    "layertest=layers.Normalization()\n",
    "layertest.adapt(x_test)\n",
    "x_test=layertest(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YAkrMeLWzHy7",
   "metadata": {
    "id": "YAkrMeLWzHy7"
   },
   "source": [
    "## 2)Preprocessing and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ZBE0BxOo5S0",
   "metadata": {
    "id": "3ZBE0BxOo5S0"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sc = StandardScaler()\n",
    "#Fit to data, then transform it.\n",
    "x_traink = sc.fit_transform(x_traink)\n",
    "#Perform standardization by centering and scaling.\n",
    "x_testk = sc.transform(x_testk)\n",
    "#Applying PCA function for n_components=2\n",
    "pca = PCA(n_components = 2)\n",
    "x_traink = pca.fit_transform(x_traink)\n",
    "x_testk = pca.transform(x_testk)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0mWm0CUSKfA9",
   "metadata": {
    "id": "0mWm0CUSKfA9"
   },
   "source": [
    "## 3)KNN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NiQbxvsZp4kt",
   "metadata": {
    "id": "NiQbxvsZp4kt"
   },
   "source": [
    "#### A)KNN with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n0ouay1KKBlc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0ouay1KKBlc",
    "outputId": "ef640921-9f49-45f4-d415-4f5b0e04783d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0685\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.12      0.18      1000\n",
      "           1       0.17      0.06      0.08      1000\n",
      "           2       0.15      0.06      0.08      1000\n",
      "           3       0.11      0.04      0.05      1000\n",
      "           4       0.13      0.04      0.06      1000\n",
      "           5       0.16      0.06      0.08      1000\n",
      "           6       0.18      0.07      0.10      1000\n",
      "           7       0.11      0.03      0.05      1000\n",
      "           8       0.23      0.10      0.14      1000\n",
      "           9       0.25      0.11      0.15      1000\n",
      "\n",
      "   micro avg       0.19      0.07      0.10     10000\n",
      "   macro avg       0.18      0.07      0.10     10000\n",
      "weighted avg       0.18      0.07      0.10     10000\n",
      " samples avg       0.07      0.07      0.07     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#setting how many neighbors to look\n",
    "neighbors=3\n",
    "#Create KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "#Train the model using the training sets\n",
    "knn.fit(x_traink, y_traink)\n",
    "#Predict for test dataset\n",
    "y_predk = knn.predict(x_testk)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_testk, y_predk))\n",
    "#Classification report\n",
    "print(\"Classification Report:\")\n",
    "cr = metrics.classification_report(y_testk, y_predk)\n",
    "print(cr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mDnG99Higec5",
   "metadata": {
    "id": "mDnG99Higec5"
   },
   "source": [
    "#### B)KNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FX9x5RHhpSoU",
   "metadata": {
    "id": "FX9x5RHhpSoU"
   },
   "outputs": [],
   "source": [
    "#predicts the label of test x_t with k-neares neighbors\n",
    "def predictKNeighbors(x_t,k):\n",
    "    #the 10 classes \n",
    "    classes = np.arange(10)\n",
    "    #counting for each class\n",
    "    classVotes =np.zeros(10)\n",
    "    neighbors=getKNeighbors(x_t,k)\n",
    "    for i in range(k):\n",
    "        p=neighbors[i]\n",
    "    if p in classes:\n",
    "        classVotes[p]+=1\n",
    "    return np.argmax(classVotes)\n",
    "#function that returns the labels of k-nearest neighbors of x_t\n",
    "def getKNeighbors(x_t,k):\n",
    "    distances=[]\n",
    "    neighbors=[]\n",
    "    for i in range(x_traink.shape[0]):\n",
    "        d=euclideanDistance(x_t,x_traink[i])\n",
    "        #append to the list distances the labels of each train example and its calculated distance from x_t\n",
    "        distances.append((np.argmax(y_traink[i]), d))\n",
    "    #sort distances by euclidean distances    \n",
    "    distances.sort(key=sort)\n",
    "    for i in range(k):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "#function that calculates euclideanDistance\n",
    "def euclideanDistance(x,y):\n",
    "    distance = 0\n",
    "    for i in range(len(x)):\n",
    "        distance += pow((x[i] - y[i]), 2)\n",
    "    return math.sqrt(distance)\n",
    "#function that helps sort list distances\n",
    "def sort(e):\n",
    "    return e[:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WhU2ed2ngdkl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhU2ed2ngdkl",
    "outputId": "17b4be04-069c-425f-c4f2-155165e25a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "Accuracy: 0.14666666666666667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.17      0.18        36\n",
      "           1       0.11      0.17      0.13        24\n",
      "           2       0.08      0.07      0.08        27\n",
      "           3       0.10      0.10      0.10        29\n",
      "           4       0.07      0.09      0.08        23\n",
      "           5       0.12      0.11      0.11        28\n",
      "           6       0.28      0.24      0.25        34\n",
      "           7       0.13      0.15      0.14        27\n",
      "           8       0.22      0.19      0.20        37\n",
      "           9       0.16      0.14      0.15        35\n",
      "\n",
      "    accuracy                           0.15       300\n",
      "   macro avg       0.15      0.14      0.14       300\n",
      "weighted avg       0.15      0.15      0.15       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predict for test dataset\n",
    "y_pred=[]\n",
    "numberOFtests=300#its too slow for whole test set so i chose a part of the test set\n",
    "for i in range(numberOFtests):\n",
    "    y_pred.append(predictKNeighbors(x_testk[i],3))\n",
    "    if((i%100)==0):\n",
    "      print(i)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(np.argmax(y_testk[0:numberOFtests],1), y_pred))\n",
    "#Classification report\n",
    "print(\"Classification Report:\")\n",
    "cr = metrics.classification_report(np.argmax(y_testk[0:numberOFtests],1), y_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3k8TJGFketD_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3k8TJGFketD_",
    "outputId": "1183f350-fc3a-4552-c9ae-782f1bfa45fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2061\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.53      0.31      1000\n",
      "           1       0.16      0.15      0.15      1000\n",
      "           2       0.18      0.07      0.10      1000\n",
      "           3       0.11      0.01      0.01      1000\n",
      "           4       0.15      0.03      0.05      1000\n",
      "           5       0.20      0.23      0.22      1000\n",
      "           6       0.18      0.50      0.26      1000\n",
      "           7       0.10      0.02      0.03      1000\n",
      "           8       0.26      0.21      0.23      1000\n",
      "           9       0.30      0.32      0.30      1000\n",
      "\n",
      "    accuracy                           0.21     10000\n",
      "   macro avg       0.19      0.21      0.17     10000\n",
      "weighted avg       0.19      0.21      0.17     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "#Create NearestCentroid Classifier\n",
    "model = NearestCentroid()\n",
    "y_train_values=np.argmax(y_traink,1)\n",
    "#Train the model using the training sets\n",
    "model.fit(x_traink, y_train_values)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",model.score(x_testk,np.argmax(y_testk,1)))\n",
    "#Predict for test dataset\n",
    "y_pred=model.predict(x_testk)\n",
    "#Classification report\n",
    "print(\"Classification Report:\")\n",
    "cr = metrics.classification_report(np.argmax(y_testk,1), y_pred)\n",
    "print(cr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XInPGvtzetsr",
   "metadata": {
    "id": "XInPGvtzetsr"
   },
   "source": [
    "## 4)Nearest Class Centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bPmoADhn8e_o",
   "metadata": {
    "id": "bPmoADhn8e_o"
   },
   "source": [
    "## Results of KNN and NCC for mnist and cifar-10 databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OJdAl38QxibG",
   "metadata": {
    "id": "OJdAl38QxibG"
   },
   "source": [
    "## 5)Building neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4jDwjWCmdr4U",
   "metadata": {
    "id": "4jDwjWCmdr4U"
   },
   "source": [
    "### Initialization of parameters using He weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "J-1UyrpYxpTT",
   "metadata": {
    "id": "J-1UyrpYxpTT"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims= array  containing the nodes of each layer of NN\n",
    "    \n",
    "    Returns:\n",
    "    parameters = dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl= weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl= bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    # number of layers in NN +1 for layer 0 that is the input\n",
    "    L = len(layer_dims) \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] =np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2.0 / layer_dims[l-1])#he weight initialization \n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))#setting bias to zero\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nMGrxOGpdSQ1",
   "metadata": {
    "id": "nMGrxOGpdSQ1"
   },
   "source": [
    "### Activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "evfJYHwrMvhL",
   "metadata": {
    "id": "evfJYHwrMvhL"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation \n",
    "    \n",
    "    Arguments:\n",
    "    Z = Output of the linear layer, of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A = output of sigmoid(z), same shape as Z\n",
    "    cache = returns Z as well, useful for backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z = Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A = output of relu(z),same shape as Z\n",
    "    cache = returns Z as well, useful for backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z \n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Implements the tanh activation \n",
    "    \n",
    "    Arguments:\n",
    "    Z = Output of the linear layer, of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A = output of tanh(z), same shape as Z\n",
    "    cache = returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    A=np.tanh(Z)\n",
    "    cache=Z\n",
    "\n",
    "    return A,cache;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nq2bnGLL-nH2",
   "metadata": {
    "id": "Nq2bnGLL-nH2"
   },
   "source": [
    "### Implementation of  linear part of a layer's forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "w3NjqBbIzKwI",
   "metadata": {
    "id": "w3NjqBbIzKwI"
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A = activations from previous layer (or input data),numpy array of shape (size of previous layer, number of examples)\n",
    "    W = weights matrix,numpy array of shape (size of current layer, size of previous layer)\n",
    "    b = bias vector,numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z = output of linear part or the input of the activation function\n",
    "    cache = a python tuple containing \"A\", \"W\" and \"b\" , useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    Z =np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Gs6tiFg-sIj",
   "metadata": {
    "id": "1Gs6tiFg-sIj"
   },
   "source": [
    "### Implementation of  forward propagation for a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rv08uEq7zecG",
   "metadata": {
    "id": "rv08uEq7zecG"
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev = activations from previous layer (or input data),numpy array of shape (size of previous layer, number of examples)\n",
    "    W = weights matrix, numpy array of shape (size of current layer, size of previous layer)\n",
    "    b = bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation = the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" or \"tanh\"\n",
    "\n",
    "    Returns:\n",
    "    A = the output of the activation function \n",
    "    cache = a python tuple containing \"linear_cache\" and \"activation_cache\",useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z,linear_cache=linear_forward(A_prev,W,b)\n",
    "        A,activation_cache=sigmoid(Z)\n",
    "        \n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z,linear_cache=linear_forward(A_prev,W,b)\n",
    "        A,activation_cache=relu(Z)\n",
    "\n",
    "    elif activation == \"tanh\":\n",
    "      Z,linear_cache=linear_forward(A_prev,W,b)\n",
    "      A,activation_cache=tanh(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0yAiisW7--Yl",
   "metadata": {
    "id": "0yAiisW7--Yl"
   },
   "source": [
    "### Implementation of forward propagation for L-1 layers with activation \"activation\" and for the last layer with sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "OCu56QEazwW-",
   "metadata": {
    "id": "OCu56QEazwW-"
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters,activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X = data, numpy array of shape (input size, number of examples)\n",
    "    parameters = initialized parameters from initialize_parameters function\n",
    "    activation=activation function for first L-1 layers stored as a text string: \"sigmoid\" or \"relu\" or \"tanh\"\n",
    "    \n",
    "    Returns:\n",
    "    AL = activation value from the output (last) layer,numpy array of shape (output size,number of examples) where output_size=number of classes\n",
    "    caches = list of caches containing every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    # number of layers in the NN ,its /2 because we have \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
    "    L = len(parameters) // 2                  \n",
    "    \n",
    "    # Implement forward propagation for L-1 layers with activation \"activation\". Add \"cache\" to the \"caches\" list.\n",
    "    # The for loop starts at 1 because layer 0 is the input\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A,cache=linear_activation_forward(A_prev,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],activation)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement forward propagation for the last  layer with sigmoid activation. Add \"cache\" to the \"caches\" list.\n",
    "    AL,cache=linear_activation_forward(A,parameters[\"W\"+str(L)],parameters[\"b\"+str(L)],\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GIQ7dEJR_HWG",
   "metadata": {
    "id": "GIQ7dEJR_HWG"
   },
   "source": [
    "### Implementation of cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wdzIfPhU0R8R",
   "metadata": {
    "id": "wdzIfPhU0R8R"
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL = probability vector corresponding to your label predictions, numpy array of shape  (number of clases, number of examples)\n",
    "    Y = true \"label\" vector , numpy array of shape (number of classes, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost = cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute cross-entropy loss from AL and Y.\n",
    "    #product= element wise multiplacation of -Y and log(AL) arrays ,array of shape (number of classes, number of examples)\n",
    "    product=-Y*np.log(AL)\n",
    "    #coste=cost for every example,array of shape (1,number of examples)\n",
    "    coste=np.sum(product,axis=0)\n",
    "    #total cost \n",
    "    cost=1/m *np.sum(coste)\n",
    "    \n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure  cost's shape is what we expect\n",
    "    \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NkzE-xlxtiBA",
   "metadata": {
    "id": "NkzE-xlxtiBA"
   },
   "source": [
    "### Calculate local gradients for different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ejz5IfrythTr",
   "metadata": {
    "id": "ejz5IfrythTr"
   },
   "outputs": [],
   "source": [
    "def relu_local_gradient(dA, cache):\n",
    "    \"\"\"\n",
    "    Calculate the local gradient for a  relu unit with respect to Z given the post-activation gradient\n",
    "\n",
    "    Arguments:\n",
    "    dA = post-activation gradient, of any shape\n",
    "    cache = 'Z' Output of the linear layer\n",
    "\n",
    "    Returns:\n",
    "    dZ =Local gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_local_gradient(dA, cache):\n",
    "    \"\"\"\n",
    "    Calculate the local gradient for a  sigmoid unit with respect to Z given the post-activation gradient.\n",
    "\n",
    "    Arguments:\n",
    "    dA = post-activation gradient, of any shape\n",
    "    cache ='Z' Output of the linear layer\n",
    "\n",
    "    Returns:\n",
    "    dZ = Local gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def tanh_local_gradient(dA,cache):\n",
    "    \"\"\"\n",
    "    Calculate the local gradient for a  tanh unit with respect to Z given the post-activation gradient.\n",
    "\n",
    "    Arguments:\n",
    "    dA = post-activation gradient, of any shape\n",
    "    cache ='Z' Output of the linear layer\n",
    "\n",
    "    Returns:\n",
    "    dZ = Local gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z=cache\n",
    "    dZ=dA*(1-np.tanh(Z)**2)\n",
    "\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mvtRsqSe8wsI",
   "metadata": {
    "id": "mvtRsqSe8wsI"
   },
   "source": [
    "### Calculate the gradients for parameters update and for previous layer back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4Rknnlf11VLD",
   "metadata": {
    "id": "4Rknnlf11VLD"
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dZ = Local gradient of the cost with respect to Z, linear output (of current layer l)\n",
    "    cache = tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev = Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW = Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db = Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW=1/m *np.matmul(dZ,A_prev.T)\n",
    "    db=1/m * np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev=np.matmul(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A07ffMzgDJcr",
   "metadata": {
    "id": "A07ffMzgDJcr"
   },
   "source": [
    "### Implementation of whole backward propagation for a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qb0zI_ct1bd1",
   "metadata": {
    "id": "qb0zI_ct1bd1"
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA = post-activation gradient for current layer l \n",
    "    cache = tuple of values (linear_cache, activation_cache)  stored for computing backward propagation \n",
    "    activation = the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" or \"tanh\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev = Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW = Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db = Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ=relu_local_gradient(dA, activation_cache)\n",
    "        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ=sigmoid_local_gradient(dA, activation_cache)\n",
    "        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n",
    "\n",
    "    elif activation == \"tanh\":\n",
    "        dZ=tanh_local_gradient(dA, activation_cache)\n",
    "        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-H5JCs6Sx4Zg",
   "metadata": {
    "id": "-H5JCs6Sx4Zg"
   },
   "source": [
    "### Implementation of backward propagation for L-1 layers with activation function and for the last with sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "X0olZdPm1woP",
   "metadata": {
    "id": "X0olZdPm1woP"
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches,activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL = probability vector corresponding to your label predictions, numpy array of shape  (number of clases, number of examples)\n",
    "    Y = true \"label\" vector , numpy array of shape (number of classes, number of examples)\n",
    "    caches = list of caches containing:every cache of linear_activation_forward() with \"activation\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    activation=activation function for first L-1 layers stored as a text string: \"sigmoid\" or \"relu\" or \"tanh\"\n",
    "    Returns:\n",
    "    grads = A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the derivative of AL,the output of lat layer\n",
    "    dAL = -np.divide(Y, AL)\n",
    "    \n",
    "    # Lth layer  gradients\n",
    "    current_cache=caches[L-1]\n",
    "    dA_prev_temp,dW_temp,db_temp=linear_activation_backward(dAL,current_cache,'sigmoid')\n",
    "    grads[\"dA\" + str(L-1)] =dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] =dW_temp\n",
    "    grads[\"db\" + str(L)] =db_temp\n",
    "\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (ACTIVATION -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache=caches[l]\n",
    "        dA_prev_temp,dW_temp,db_temp=linear_activation_backward(dA_prev_temp,current_cache,activation)\n",
    "        grads[\"dA\" + str(l)] =dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] =dW_temp\n",
    "        grads[\"db\" + str(l+1)] =db_temp\n",
    "        \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2WqQY13NfnpB",
   "metadata": {
    "id": "2WqQY13NfnpB"
   },
   "source": [
    "### Update parameters using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "zHn199F32qGD",
   "metadata": {
    "id": "zHn199F32qGD"
   },
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    params= python dictionary containing your parameters \n",
    "    grads = python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters = python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    # number of layers in the NN ,its /2 because we have \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)]= parameters[\"W\"+str(l+1)] + learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\"+str(l+1)]=parameters[\"b\"+str(l+1)] +learning_rate *grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lbUSHy80gFxM",
   "metadata": {
    "id": "lbUSHy80gFxM"
   },
   "source": [
    "### Implementation  of a L-layer NN for classification with L-1 layers with \"activation\" function and the last layer with sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pot2YGpU3EA3",
   "metadata": {
    "id": "pot2YGpU3EA3"
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims,activation, learning_rate = 0.0075, num_iterations = 3000, print_cost=False,):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X = data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y = true \"label\" vector , of shape (number of classes, number of examples)\n",
    "    layers_dims = list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    activation= activation function for first L-1 layers stored as a text string: \"sigmoid\" or \"relu\" or \"tanh\"\n",
    "    learning_rate = learning rate of the gradient descent update rule\n",
    "    num_iterations = number of iterations of the optimization loop\n",
    "    print_cost = if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters = parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters=initialize_parameters(layers_dims)\n",
    "    \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> activation]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL,caches=L_model_forward(X,parameters,activation)\n",
    "        \n",
    "        \n",
    "        # Compute cost.\n",
    "        cost=compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads= L_model_backward(AL, Y, caches,activation)\n",
    "        \n",
    " \n",
    "        # Update parameters.\n",
    "        parameters=update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "i7wNsMi33S7H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i7wNsMi33S7H",
    "outputId": "7a55e4d1-3a7f-40e6-f3e4-4a4fbdda4881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6779928364910706\n",
      "Cost after iteration 100: 8.902473555239633\n",
      "Cost after iteration 200: 25.15117719558575\n",
      "Cost after iteration 300: 42.674405346885216\n",
      "Cost after iteration 400: 60.50712155583745\n",
      "Cost after iteration 500: 78.95740053848482\n",
      "Cost after iteration 600: 97.66834962712555\n",
      "Cost after iteration 700: 116.5499480781811\n",
      "Cost after iteration 800: 135.4511084749548\n",
      "Cost after iteration 900: 154.35124793798948\n",
      "Cost after iteration 999: 173.06236093599094\n",
      "Accuracy: 0.1\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      5000\n",
      "           1       0.00      0.00      0.00      5000\n",
      "           2       0.00      0.00      0.00      5000\n",
      "           3       0.00      0.00      0.00      5000\n",
      "           4       0.10      1.00      0.18      5000\n",
      "           5       0.00      0.00      0.00      5000\n",
      "           6       0.00      0.00      0.00      5000\n",
      "           7       0.00      0.00      0.00      5000\n",
      "           8       0.00      0.00      0.00      5000\n",
      "           9       0.00      0.00      0.00      5000\n",
      "\n",
      "    accuracy                           0.10     50000\n",
      "   macro avg       0.01      0.10      0.02     50000\n",
      "weighted avg       0.01      0.10      0.02     50000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\michail dadopoulos\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\michail dadopoulos\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\michail dadopoulos\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1000\n",
      "           1       0.00      0.00      0.00      1000\n",
      "           2       0.00      0.00      0.00      1000\n",
      "           3       0.00      0.00      0.00      1000\n",
      "           4       0.10      1.00      0.18      1000\n",
      "           5       0.00      0.00      0.00      1000\n",
      "           6       0.00      0.00      0.00      1000\n",
      "           7       0.00      0.00      0.00      1000\n",
      "           8       0.00      0.00      0.00      1000\n",
      "           9       0.00      0.00      0.00      1000\n",
      "\n",
      "    accuracy                           0.10     10000\n",
      "   macro avg       0.01      0.10      0.02     10000\n",
      "weighted avg       0.01      0.10      0.02     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\michail dadopoulos\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\michail dadopoulos\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\michail dadopoulos\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_costs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m cr \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mclassification_report(np\u001b[38;5;241m.\u001b[39margmax(y_test,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), y_test_pred)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(cr)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mplot_costs\u001b[49m(costs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_costs' is not defined"
     ]
    }
   ],
   "source": [
    "#layers_dims = [784, 64, 32, 16, 10]#for Mnist dataset\n",
    "layers_dims = [3072, 256, 64,  10] #for cifar10 dataset\n",
    "activation=\"sigmoid\"\n",
    "parameters, costs = L_layer_model(x_train, y_train,layers_dims,activation,learning_rate=0.03, num_iterations = 1000, print_cost = True)\n",
    "#pred_train = predict(x_train,y_train, parameters,activation)\n",
    "#pred_test = predict(x_test, y_test, parameters,activation)\n",
    "y_train_pred, caches = L_model_forward(x_train, parameters,activation)\n",
    "# convert probas to 0-10 prediction\n",
    "y_train_pred = np.argmax(y_train_pred,axis=0)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(np.argmax(y_train,axis=0), y_train_pred))\n",
    "#Classification report\n",
    "print(\"Classification Report:\")\n",
    "cr = metrics.classification_report(np.argmax(y_train,axis=0), y_train_pred)\n",
    "print(cr)\n",
    "y_test_pred, caches = L_model_forward(x_test, parameters,activation)\n",
    "# convert probas to 0-10 prediction\n",
    "y_test_pred = np.argmax(y_test_pred,axis=0)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(np.argmax(y_test,axis=0), y_test_pred))\n",
    "#Classification report\n",
    "print(\"Classification Report:\")\n",
    "cr = metrics.classification_report(np.argmax(y_test,axis=0), y_test_pred)\n",
    "print(cr)\n",
    "plot_costs(costs)\n",
    "#print_mislabeled_images(classes,x_test, y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5gUehf-MUlU2",
   "metadata": {
    "id": "5gUehf-MUlU2"
   },
   "source": [
    "### Results for layers [3072, 256, 128, 64, 32, 10],cifar10 dataset,learning rate=0.03,iterations 2000,5 hour and 1 minutes training ,with he initialization,full batch,with normalization,relu activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hKk450-bUYUw",
   "metadata": {
    "id": "hKk450-bUYUw"
   },
   "source": [
    "Cost after iteration 0: 0.9317825369690396\\\n",
    "Cost after iteration 1999: 0.00010914024495493789\\\n",
    "Train Accuracy: 0.17506\\\n",
    "Test Accuracy: 0.1709\\\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5KFHfuVL-SAi",
   "metadata": {
    "id": "5KFHfuVL-SAi"
   },
   "source": [
    "### Results for layers [3072, 256, 128, 64, 32, 10],cifar10 dataset,learning rate=0.0075,iterations 2500,6 hour and 10 minutes training ,with he initialization,full batch,with normalization,relu activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mo2ADORm-bXJ",
   "metadata": {
    "id": "Mo2ADORm-bXJ"
   },
   "source": [
    "Cost after iteration 0: 0.9317825369690396\\\n",
    "Cost after iteration 2499: 0.000486231319594058\\\n",
    "Train Accuracy: 0.17238000000000003\\\n",
    "Test Accuracy: 0.16510000000000002\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FZ2dN5-T34cN",
   "metadata": {
    "id": "FZ2dN5-T34cN"
   },
   "source": [
    "### Results for layers [784, 64, 32, 16, 10] ,mnist dataset, learning rate=0.0075,iterations 2500,1 hour and 30 minutes training ,with he initialization,full batch,no normalization,relu activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rE5-jpKD30j3",
   "metadata": {
    "id": "rE5-jpKD30j3"
   },
   "source": [
    "Cost after iteration 0: 0.7534273536560523\\\n",
    "Cost after iteration 2499: 0.041767638716373165\\\n",
    "Accuracy: 0.4391000000000001\\\n",
    "Accuracy: 0.4353000000000001\n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmSAqQENVMMI",
   "metadata": {
    "id": "kmSAqQENVMMI"
   },
   "source": [
    "### Evaluation:\n",
    "It seems to be better and better with more iterations but it need too much time,learning rate=0.03 was better than 0.0075"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DuoJi8HRrG1h",
   "metadata": {
    "id": "DuoJi8HRrG1h"
   },
   "source": [
    "### Keras Implementation of Neural Network build from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dPC8d97XrFVe",
   "metadata": {
    "id": "dPC8d97XrFVe"
   },
   "outputs": [],
   "source": [
    "def simple_nn():\n",
    "  \"\"\"\n",
    "  Implements a simple full Dense NN with 5 layers\n",
    "\n",
    "  Arguments:\n",
    "  None\n",
    "\n",
    "  Returns:\n",
    "  model= Keras Sequential model \n",
    "  \"\"\"\n",
    "  model = Sequential() \n",
    "  model.add(Dense(units=256, activation='relu', input_shape=(3072,)))\n",
    "  model.add(Dense(units=128, activation='relu'))\n",
    "  model.add(Dense(units=64, activation='relu'))\n",
    "  model.add(Dense(units=32, activation='relu'))\n",
    "  model.add(Dense(units=10, activation='sigmoid'))\n",
    "\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70kEo6oyuG5o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70kEo6oyuG5o",
    "outputId": "e3aa537f-7668-43e3-9751-43005568fd61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 830,250\n",
      "Trainable params: 830,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.7776 - accuracy: 0.3697\n",
      "Epoch 2/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5460 - accuracy: 0.4526\n",
      "Epoch 3/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4423 - accuracy: 0.4872\n",
      "Epoch 4/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.3686 - accuracy: 0.5186\n",
      "Epoch 5/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.2967 - accuracy: 0.5429\n",
      "Epoch 6/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2422 - accuracy: 0.5618\n",
      "Epoch 7/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1907 - accuracy: 0.5797\n",
      "Epoch 8/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.1388 - accuracy: 0.5959\n",
      "Epoch 9/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0939 - accuracy: 0.6126\n",
      "Epoch 10/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.0531 - accuracy: 0.6235\n",
      "Epoch 11/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.0079 - accuracy: 0.6438\n",
      "Epoch 12/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.9726 - accuracy: 0.6555\n",
      "Epoch 13/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.9391 - accuracy: 0.6675\n",
      "Epoch 14/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.8984 - accuracy: 0.6805\n",
      "Epoch 15/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.8708 - accuracy: 0.6920\n",
      "Epoch 16/500\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.8400 - accuracy: 0.7007\n",
      "Epoch 17/500\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.8082 - accuracy: 0.7140\n",
      "Epoch 18/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7848 - accuracy: 0.7218\n",
      "Epoch 19/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7534 - accuracy: 0.7323\n",
      "Epoch 20/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.7348 - accuracy: 0.7395\n",
      "Epoch 21/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.7122 - accuracy: 0.7472\n",
      "Epoch 22/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.6917 - accuracy: 0.7548\n",
      "Epoch 23/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.6707 - accuracy: 0.7627\n",
      "Epoch 24/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.6475 - accuracy: 0.7714\n",
      "Epoch 25/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.6309 - accuracy: 0.7798\n",
      "Epoch 26/500\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.6030 - accuracy: 0.7861\n",
      "Epoch 27/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5923 - accuracy: 0.7905\n",
      "Epoch 28/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5767 - accuracy: 0.7952\n",
      "Epoch 29/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5616 - accuracy: 0.8019\n",
      "Epoch 30/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5428 - accuracy: 0.8079\n",
      "Epoch 31/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.5345 - accuracy: 0.8106\n",
      "Epoch 32/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.5227 - accuracy: 0.8159\n",
      "Epoch 33/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5114 - accuracy: 0.8184\n",
      "Epoch 34/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4940 - accuracy: 0.8237\n",
      "Epoch 35/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4849 - accuracy: 0.8289\n",
      "Epoch 36/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4717 - accuracy: 0.8343\n",
      "Epoch 37/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4549 - accuracy: 0.8412\n",
      "Epoch 38/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4554 - accuracy: 0.8397\n",
      "Epoch 39/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4434 - accuracy: 0.8434\n",
      "Epoch 40/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4297 - accuracy: 0.8486\n",
      "Epoch 41/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4274 - accuracy: 0.8505\n",
      "Epoch 42/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4113 - accuracy: 0.8567\n",
      "Epoch 43/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4050 - accuracy: 0.8586\n",
      "Epoch 44/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4055 - accuracy: 0.8593\n",
      "Epoch 45/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3872 - accuracy: 0.8666\n",
      "Epoch 46/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.3894 - accuracy: 0.8644\n",
      "Epoch 47/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3632 - accuracy: 0.8737\n",
      "Epoch 48/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3736 - accuracy: 0.8704\n",
      "Epoch 49/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.3673 - accuracy: 0.8737\n",
      "Epoch 50/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.3554 - accuracy: 0.8776\n",
      "Epoch 51/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.3554 - accuracy: 0.8769\n",
      "Epoch 52/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.3389 - accuracy: 0.8830\n",
      "Epoch 53/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3374 - accuracy: 0.8853\n",
      "Epoch 54/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3395 - accuracy: 0.8834\n",
      "Epoch 55/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3252 - accuracy: 0.8881\n",
      "Epoch 56/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3212 - accuracy: 0.8897\n",
      "Epoch 57/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3164 - accuracy: 0.8913\n",
      "Epoch 58/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.3146 - accuracy: 0.8933\n",
      "Epoch 59/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3041 - accuracy: 0.8963\n",
      "Epoch 60/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3006 - accuracy: 0.8970\n",
      "Epoch 61/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2920 - accuracy: 0.9007\n",
      "Epoch 62/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2978 - accuracy: 0.8980\n",
      "Epoch 63/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2814 - accuracy: 0.9041\n",
      "Epoch 64/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2911 - accuracy: 0.9018\n",
      "Epoch 65/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2822 - accuracy: 0.9047\n",
      "Epoch 66/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2746 - accuracy: 0.9075\n",
      "Epoch 67/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2733 - accuracy: 0.9085\n",
      "Epoch 68/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.2702 - accuracy: 0.9093\n",
      "Epoch 69/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2830 - accuracy: 0.9052\n",
      "Epoch 70/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2531 - accuracy: 0.9141\n",
      "Epoch 71/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2622 - accuracy: 0.9112\n",
      "Epoch 72/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2518 - accuracy: 0.9146\n",
      "Epoch 73/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2572 - accuracy: 0.9125\n",
      "Epoch 74/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2605 - accuracy: 0.9132\n",
      "Epoch 75/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2450 - accuracy: 0.9169\n",
      "Epoch 76/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2586 - accuracy: 0.9144\n",
      "Epoch 77/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.2423 - accuracy: 0.9187\n",
      "Epoch 78/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2436 - accuracy: 0.9196\n",
      "Epoch 79/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2363 - accuracy: 0.9220\n",
      "Epoch 80/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.2323 - accuracy: 0.9237\n",
      "Epoch 81/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2342 - accuracy: 0.9213\n",
      "Epoch 82/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2221 - accuracy: 0.9266\n",
      "Epoch 83/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2300 - accuracy: 0.9232\n",
      "Epoch 84/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2316 - accuracy: 0.9234\n",
      "Epoch 85/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2169 - accuracy: 0.9270\n",
      "Epoch 86/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2166 - accuracy: 0.9285\n",
      "Epoch 87/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2173 - accuracy: 0.9276\n",
      "Epoch 88/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.2188 - accuracy: 0.9288\n",
      "Epoch 89/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2145 - accuracy: 0.9283\n",
      "Epoch 90/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.2093 - accuracy: 0.9306\n",
      "Epoch 91/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.2072 - accuracy: 0.9302\n",
      "Epoch 92/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.2158 - accuracy: 0.9298\n",
      "Epoch 93/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2002 - accuracy: 0.9349\n",
      "Epoch 94/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1976 - accuracy: 0.9344\n",
      "Epoch 95/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2093 - accuracy: 0.9312\n",
      "Epoch 96/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2074 - accuracy: 0.9320\n",
      "Epoch 97/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1883 - accuracy: 0.9371\n",
      "Epoch 98/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2010 - accuracy: 0.9342\n",
      "Epoch 99/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1963 - accuracy: 0.9359\n",
      "Epoch 100/500\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.1900 - accuracy: 0.9375\n",
      "Epoch 101/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2035 - accuracy: 0.9347\n",
      "Epoch 102/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1848 - accuracy: 0.9393\n",
      "Epoch 103/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1859 - accuracy: 0.9388\n",
      "Epoch 104/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1881 - accuracy: 0.9388\n",
      "Epoch 105/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1990 - accuracy: 0.9352\n",
      "Epoch 106/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1763 - accuracy: 0.9425\n",
      "Epoch 107/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1865 - accuracy: 0.9377\n",
      "Epoch 108/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2006 - accuracy: 0.9356\n",
      "Epoch 109/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1926 - accuracy: 0.9387\n",
      "Epoch 110/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1655 - accuracy: 0.9458\n",
      "Epoch 111/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1861 - accuracy: 0.9406\n",
      "Epoch 112/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1728 - accuracy: 0.9437\n",
      "Epoch 113/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1806 - accuracy: 0.9421\n",
      "Epoch 114/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1552 - accuracy: 0.9503\n",
      "Epoch 115/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1839 - accuracy: 0.9415\n",
      "Epoch 116/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1662 - accuracy: 0.9468\n",
      "Epoch 117/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1805 - accuracy: 0.9420\n",
      "Epoch 118/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1666 - accuracy: 0.9470\n",
      "Epoch 119/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1611 - accuracy: 0.9475\n",
      "Epoch 120/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1820 - accuracy: 0.9428\n",
      "Epoch 121/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1677 - accuracy: 0.9476\n",
      "Epoch 122/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1640 - accuracy: 0.9483\n",
      "Epoch 123/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1713 - accuracy: 0.9457\n",
      "Epoch 124/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1695 - accuracy: 0.9462\n",
      "Epoch 125/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1519 - accuracy: 0.9516\n",
      "Epoch 126/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1732 - accuracy: 0.9442\n",
      "Epoch 127/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1599 - accuracy: 0.9493\n",
      "Epoch 128/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1638 - accuracy: 0.9472\n",
      "Epoch 129/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1486 - accuracy: 0.9528\n",
      "Epoch 130/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1611 - accuracy: 0.9496\n",
      "Epoch 131/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1551 - accuracy: 0.9505\n",
      "Epoch 132/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1576 - accuracy: 0.9507\n",
      "Epoch 133/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1540 - accuracy: 0.9521\n",
      "Epoch 134/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1533 - accuracy: 0.9517\n",
      "Epoch 135/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1560 - accuracy: 0.9508\n",
      "Epoch 136/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1422 - accuracy: 0.9541\n",
      "Epoch 137/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1469 - accuracy: 0.9522\n",
      "Epoch 138/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1981 - accuracy: 0.9411\n",
      "Epoch 139/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1392 - accuracy: 0.9563\n",
      "Epoch 140/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1439 - accuracy: 0.9550\n",
      "Epoch 141/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1465 - accuracy: 0.9545\n",
      "Epoch 142/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1713 - accuracy: 0.9474\n",
      "Epoch 143/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1181 - accuracy: 0.9621\n",
      "Epoch 144/500\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.1645 - accuracy: 0.9495\n",
      "Epoch 145/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1359 - accuracy: 0.9573\n",
      "Epoch 146/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1505 - accuracy: 0.9532\n",
      "Epoch 147/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1398 - accuracy: 0.9557\n",
      "Epoch 148/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1461 - accuracy: 0.9547\n",
      "Epoch 149/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1729 - accuracy: 0.9489\n",
      "Epoch 150/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1335 - accuracy: 0.9584\n",
      "Epoch 151/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1371 - accuracy: 0.9570\n",
      "Epoch 152/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1447 - accuracy: 0.9547\n",
      "Epoch 153/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1635 - accuracy: 0.9519\n",
      "Epoch 154/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1323 - accuracy: 0.9593\n",
      "Epoch 155/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1432 - accuracy: 0.9566\n",
      "Epoch 156/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1315 - accuracy: 0.9588\n",
      "Epoch 157/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1365 - accuracy: 0.9568\n",
      "Epoch 158/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1435 - accuracy: 0.9563\n",
      "Epoch 159/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1309 - accuracy: 0.9597\n",
      "Epoch 160/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1306 - accuracy: 0.9597\n",
      "Epoch 161/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1389 - accuracy: 0.9590\n",
      "Epoch 162/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1278 - accuracy: 0.9595\n",
      "Epoch 163/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.1372 - accuracy: 0.9577\n",
      "Epoch 164/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1242 - accuracy: 0.9606\n",
      "Epoch 165/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1505 - accuracy: 0.9549\n",
      "Epoch 166/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1462 - accuracy: 0.9558\n",
      "Epoch 167/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1154 - accuracy: 0.9644\n",
      "Epoch 168/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1269 - accuracy: 0.9601\n",
      "Epoch 169/500\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.1352 - accuracy: 0.9593\n",
      "Epoch 170/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1325 - accuracy: 0.9592\n",
      "Epoch 171/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1275 - accuracy: 0.9596\n",
      "Epoch 172/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1333 - accuracy: 0.9593\n",
      "Epoch 173/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1229 - accuracy: 0.9620\n",
      "Epoch 174/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1688 - accuracy: 0.9492\n",
      "Epoch 175/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1117 - accuracy: 0.9663\n",
      "Epoch 176/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1286 - accuracy: 0.9608\n",
      "Epoch 177/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1551 - accuracy: 0.9528\n",
      "Epoch 178/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1518 - accuracy: 0.9555\n",
      "Epoch 179/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1132 - accuracy: 0.9644\n",
      "Epoch 180/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1244 - accuracy: 0.9629\n",
      "Epoch 181/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1214 - accuracy: 0.9630\n",
      "Epoch 182/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1263 - accuracy: 0.9607\n",
      "Epoch 183/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1199 - accuracy: 0.9630\n",
      "Epoch 184/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1585 - accuracy: 0.9561\n",
      "Epoch 185/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1236 - accuracy: 0.9635\n",
      "Epoch 186/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1540 - accuracy: 0.9556\n",
      "Epoch 187/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1216 - accuracy: 0.9634\n",
      "Epoch 188/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1061 - accuracy: 0.9679\n",
      "Epoch 189/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1179 - accuracy: 0.9638\n",
      "Epoch 190/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1235 - accuracy: 0.9625\n",
      "Epoch 191/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1163 - accuracy: 0.9651\n",
      "Epoch 192/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1204 - accuracy: 0.9641\n",
      "Epoch 193/500\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.1226 - accuracy: 0.9621\n",
      "Epoch 194/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1181 - accuracy: 0.9634\n",
      "Epoch 195/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1216 - accuracy: 0.9638\n",
      "Epoch 196/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1241 - accuracy: 0.9638\n",
      "Epoch 197/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1073 - accuracy: 0.9677\n",
      "Epoch 198/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1328 - accuracy: 0.9618\n",
      "Epoch 199/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1121 - accuracy: 0.9661\n",
      "Epoch 200/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1135 - accuracy: 0.9652\n",
      "Epoch 201/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1207 - accuracy: 0.9644\n",
      "Epoch 202/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1200 - accuracy: 0.9639\n",
      "Epoch 203/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1236 - accuracy: 0.9629\n",
      "Epoch 204/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1534 - accuracy: 0.9563\n",
      "Epoch 205/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1060 - accuracy: 0.9679\n",
      "Epoch 206/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1187 - accuracy: 0.9644\n",
      "Epoch 207/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1041 - accuracy: 0.9677\n",
      "Epoch 208/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1359 - accuracy: 0.9608\n",
      "Epoch 209/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1348 - accuracy: 0.9606\n",
      "Epoch 210/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1005 - accuracy: 0.9695\n",
      "Epoch 211/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1392 - accuracy: 0.9595\n",
      "Epoch 212/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1403 - accuracy: 0.9604\n",
      "Epoch 213/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0982 - accuracy: 0.9706\n",
      "Epoch 214/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1037 - accuracy: 0.9692\n",
      "Epoch 215/500\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.1169 - accuracy: 0.9659\n",
      "Epoch 216/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1305 - accuracy: 0.9616\n",
      "Epoch 217/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1121 - accuracy: 0.9669\n",
      "Epoch 218/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1067 - accuracy: 0.9688\n",
      "Epoch 219/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1183 - accuracy: 0.9657\n",
      "Epoch 220/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1266 - accuracy: 0.9635\n",
      "Epoch 221/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0991 - accuracy: 0.9702\n",
      "Epoch 222/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1340 - accuracy: 0.9609\n",
      "Epoch 223/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1193 - accuracy: 0.9666\n",
      "Epoch 224/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1154 - accuracy: 0.9651\n",
      "Epoch 225/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1203 - accuracy: 0.9640\n",
      "Epoch 226/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1321 - accuracy: 0.9613\n",
      "Epoch 227/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0949 - accuracy: 0.9696\n",
      "Epoch 228/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1182 - accuracy: 0.9659\n",
      "Epoch 229/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1010 - accuracy: 0.9698\n",
      "Epoch 230/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1320 - accuracy: 0.9618\n",
      "Epoch 231/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1000 - accuracy: 0.9700\n",
      "Epoch 232/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1075 - accuracy: 0.9675\n",
      "Epoch 233/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1065 - accuracy: 0.9684\n",
      "Epoch 234/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1477 - accuracy: 0.9576\n",
      "Epoch 235/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0977 - accuracy: 0.9707\n",
      "Epoch 236/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1319 - accuracy: 0.9615\n",
      "Epoch 237/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1104 - accuracy: 0.9671\n",
      "Epoch 238/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0979 - accuracy: 0.9707\n",
      "Epoch 239/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1207 - accuracy: 0.9644\n",
      "Epoch 240/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1422 - accuracy: 0.9606\n",
      "Epoch 241/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1043 - accuracy: 0.9680\n",
      "Epoch 242/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1041 - accuracy: 0.9694\n",
      "Epoch 243/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0933 - accuracy: 0.9727\n",
      "Epoch 244/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1351 - accuracy: 0.9625\n",
      "Epoch 245/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1112 - accuracy: 0.9681\n",
      "Epoch 246/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.1019 - accuracy: 0.9710\n",
      "Epoch 247/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0952 - accuracy: 0.9728\n",
      "Epoch 248/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1053 - accuracy: 0.9684\n",
      "Epoch 249/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1079 - accuracy: 0.9686\n",
      "Epoch 250/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1227 - accuracy: 0.9646\n",
      "Epoch 251/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0912 - accuracy: 0.9734\n",
      "Epoch 252/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1026 - accuracy: 0.9703\n",
      "Epoch 253/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1233 - accuracy: 0.9645\n",
      "Epoch 254/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0852 - accuracy: 0.9741\n",
      "Epoch 255/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1148 - accuracy: 0.9675\n",
      "Epoch 256/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1399 - accuracy: 0.9619\n",
      "Epoch 257/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1075 - accuracy: 0.9703\n",
      "Epoch 258/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1092 - accuracy: 0.9691\n",
      "Epoch 259/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0853 - accuracy: 0.9747\n",
      "Epoch 260/500\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.1141 - accuracy: 0.9675\n",
      "Epoch 261/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1231 - accuracy: 0.9654\n",
      "Epoch 262/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1068 - accuracy: 0.9691\n",
      "Epoch 263/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0945 - accuracy: 0.9720\n",
      "Epoch 264/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1028 - accuracy: 0.9711\n",
      "Epoch 265/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0795 - accuracy: 0.9762\n",
      "Epoch 266/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1083 - accuracy: 0.9685\n",
      "Epoch 267/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1261 - accuracy: 0.9639\n",
      "Epoch 268/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0974 - accuracy: 0.9728\n",
      "Epoch 269/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0895 - accuracy: 0.9745\n",
      "Epoch 270/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1017 - accuracy: 0.9727\n",
      "Epoch 271/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1130 - accuracy: 0.9667\n",
      "Epoch 272/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0925 - accuracy: 0.9742\n",
      "Epoch 273/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0957 - accuracy: 0.9723\n",
      "Epoch 274/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1163 - accuracy: 0.9661\n",
      "Epoch 275/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0793 - accuracy: 0.9774\n",
      "Epoch 276/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1682 - accuracy: 0.9542\n",
      "Epoch 277/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0683 - accuracy: 0.9797\n",
      "Epoch 278/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1129 - accuracy: 0.9691\n",
      "Epoch 279/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1059 - accuracy: 0.9696\n",
      "Epoch 280/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0848 - accuracy: 0.9749\n",
      "Epoch 281/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0949 - accuracy: 0.9733\n",
      "Epoch 282/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1022 - accuracy: 0.9712\n",
      "Epoch 283/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0819 - accuracy: 0.9765\n",
      "Epoch 284/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1192 - accuracy: 0.9665\n",
      "Epoch 285/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0909 - accuracy: 0.9742\n",
      "Epoch 286/500\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1039 - accuracy: 0.9713\n",
      "Epoch 287/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1059 - accuracy: 0.9705\n",
      "Epoch 288/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0830 - accuracy: 0.9755\n",
      "Epoch 289/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1143 - accuracy: 0.9687\n",
      "Epoch 290/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0777 - accuracy: 0.9781\n",
      "Epoch 291/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1134 - accuracy: 0.9697\n",
      "Epoch 292/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1313 - accuracy: 0.9654\n",
      "Epoch 293/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1209 - accuracy: 0.9689\n",
      "Epoch 294/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0738 - accuracy: 0.9798\n",
      "Epoch 295/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1804 - accuracy: 0.9584\n",
      "Epoch 296/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0878 - accuracy: 0.9750\n",
      "Epoch 297/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1154 - accuracy: 0.9692\n",
      "Epoch 298/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0979 - accuracy: 0.9729\n",
      "Epoch 299/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.1448 - accuracy: 0.9603\n",
      "Epoch 300/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0776 - accuracy: 0.9770\n",
      "Epoch 301/500\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.0879 - accuracy: 0.9756\n",
      "Epoch 302/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1338 - accuracy: 0.9648\n",
      "Epoch 303/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0882 - accuracy: 0.9760\n",
      "Epoch 304/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1004 - accuracy: 0.9727\n",
      "Epoch 305/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1196 - accuracy: 0.9667\n",
      "Epoch 306/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0836 - accuracy: 0.9780\n",
      "Epoch 307/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0992 - accuracy: 0.9733\n",
      "Epoch 308/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1026 - accuracy: 0.9720\n",
      "Epoch 309/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0798 - accuracy: 0.9773\n",
      "Epoch 310/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0938 - accuracy: 0.9735\n",
      "Epoch 311/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0944 - accuracy: 0.9738\n",
      "Epoch 312/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0805 - accuracy: 0.9775\n",
      "Epoch 313/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0927 - accuracy: 0.9741\n",
      "Epoch 314/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1112 - accuracy: 0.9701\n",
      "Epoch 315/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0773 - accuracy: 0.9789\n",
      "Epoch 316/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1056 - accuracy: 0.9714\n",
      "Epoch 317/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1298 - accuracy: 0.9663\n",
      "Epoch 318/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0687 - accuracy: 0.9803\n",
      "Epoch 319/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0968 - accuracy: 0.9733\n",
      "Epoch 320/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0924 - accuracy: 0.9746\n",
      "Epoch 321/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1190 - accuracy: 0.9697\n",
      "Epoch 322/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1359 - accuracy: 0.9673\n",
      "Epoch 323/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0786 - accuracy: 0.9775\n",
      "Epoch 324/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0786 - accuracy: 0.9779\n",
      "Epoch 325/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1304 - accuracy: 0.9671\n",
      "Epoch 326/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0745 - accuracy: 0.9792\n",
      "Epoch 327/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0818 - accuracy: 0.9767\n",
      "Epoch 328/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0910 - accuracy: 0.9753\n",
      "Epoch 329/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0864 - accuracy: 0.9769\n",
      "Epoch 330/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1305 - accuracy: 0.9648\n",
      "Epoch 331/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1087 - accuracy: 0.9717\n",
      "Epoch 332/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0726 - accuracy: 0.9796\n",
      "Epoch 333/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0890 - accuracy: 0.9761\n",
      "Epoch 334/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0949 - accuracy: 0.9742\n",
      "Epoch 335/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0893 - accuracy: 0.9748\n",
      "Epoch 336/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0835 - accuracy: 0.9766\n",
      "Epoch 337/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0807 - accuracy: 0.9774\n",
      "Epoch 338/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1212 - accuracy: 0.9684\n",
      "Epoch 339/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1577 - accuracy: 0.9577\n",
      "Epoch 340/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0590 - accuracy: 0.9833\n",
      "Epoch 341/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0868 - accuracy: 0.9772\n",
      "Epoch 342/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1128 - accuracy: 0.9706\n",
      "Epoch 343/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1434 - accuracy: 0.9607\n",
      "Epoch 344/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0513 - accuracy: 0.9850\n",
      "Epoch 345/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0837 - accuracy: 0.9768\n",
      "Epoch 346/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0883 - accuracy: 0.9749\n",
      "Epoch 347/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0897 - accuracy: 0.9755\n",
      "Epoch 348/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0768 - accuracy: 0.9778\n",
      "Epoch 349/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1001 - accuracy: 0.9728\n",
      "Epoch 350/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0980 - accuracy: 0.9750\n",
      "Epoch 351/500\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0713 - accuracy: 0.9802\n",
      "Epoch 352/500\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0884 - accuracy: 0.9769\n",
      "Epoch 353/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1002 - accuracy: 0.9732\n",
      "Epoch 354/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1085 - accuracy: 0.9719\n",
      "Epoch 355/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1410 - accuracy: 0.9657\n",
      "Epoch 356/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0712 - accuracy: 0.9801\n",
      "Epoch 357/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0742 - accuracy: 0.9798\n",
      "Epoch 358/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1126 - accuracy: 0.9696\n",
      "Epoch 359/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0698 - accuracy: 0.9806\n",
      "Epoch 360/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0951 - accuracy: 0.9741\n",
      "Epoch 361/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.1416 - accuracy: 0.9647\n",
      "Epoch 362/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0725 - accuracy: 0.9792\n",
      "Epoch 363/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0808 - accuracy: 0.9784\n",
      "Epoch 364/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0968 - accuracy: 0.9746\n",
      "Epoch 365/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0946 - accuracy: 0.9746\n",
      "Epoch 366/500\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.0817 - accuracy: 0.9775\n",
      "Epoch 367/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0844 - accuracy: 0.9772\n",
      "Epoch 368/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0844 - accuracy: 0.9776\n",
      "Epoch 369/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0807 - accuracy: 0.9780\n",
      "Epoch 370/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0890 - accuracy: 0.9764\n",
      "Epoch 371/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0936 - accuracy: 0.9758\n",
      "Epoch 372/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0786 - accuracy: 0.9790\n",
      "Epoch 373/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0918 - accuracy: 0.9759\n",
      "Epoch 374/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1224 - accuracy: 0.9707\n",
      "Epoch 375/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.1015 - accuracy: 0.9739\n",
      "Epoch 376/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0767 - accuracy: 0.9786\n",
      "Epoch 377/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1064 - accuracy: 0.9726\n",
      "Epoch 378/500\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0980 - accuracy: 0.9737\n",
      "Epoch 379/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0749 - accuracy: 0.9797\n",
      "Epoch 380/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.1166 - accuracy: 0.9701\n",
      "Epoch 381/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0697 - accuracy: 0.9811\n",
      "Epoch 382/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0939 - accuracy: 0.9757\n",
      "Epoch 383/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0827 - accuracy: 0.9792\n",
      "Epoch 384/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0923 - accuracy: 0.9754\n",
      "Epoch 385/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0701 - accuracy: 0.9804\n",
      "Epoch 386/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0828 - accuracy: 0.9781\n",
      "Epoch 387/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0991 - accuracy: 0.9748\n",
      "Epoch 388/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0822 - accuracy: 0.9787\n",
      "Epoch 389/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0808 - accuracy: 0.9792\n",
      "Epoch 390/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1116 - accuracy: 0.9707\n",
      "Epoch 391/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0763 - accuracy: 0.9794\n",
      "Epoch 392/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0784 - accuracy: 0.9788\n",
      "Epoch 393/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0841 - accuracy: 0.9778\n",
      "Epoch 394/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0830 - accuracy: 0.9784\n",
      "Epoch 395/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1318 - accuracy: 0.9672\n",
      "Epoch 396/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0846 - accuracy: 0.9799\n",
      "Epoch 397/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0826 - accuracy: 0.9781\n",
      "Epoch 398/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1138 - accuracy: 0.9774\n",
      "Epoch 399/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1113 - accuracy: 0.9698\n",
      "Epoch 400/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0801 - accuracy: 0.9799\n",
      "Epoch 401/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0870 - accuracy: 0.9771\n",
      "Epoch 402/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0689 - accuracy: 0.9814\n",
      "Epoch 403/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1014 - accuracy: 0.9739\n",
      "Epoch 404/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0714 - accuracy: 0.9808\n",
      "Epoch 405/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0799 - accuracy: 0.9799\n",
      "Epoch 406/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1597 - accuracy: 0.9603\n",
      "Epoch 407/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0496 - accuracy: 0.9860\n",
      "Epoch 408/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0737 - accuracy: 0.9798\n",
      "Epoch 409/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0903 - accuracy: 0.9761\n",
      "Epoch 410/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.1137 - accuracy: 0.9697\n",
      "Epoch 411/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0842 - accuracy: 0.9810\n",
      "Epoch 412/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0829 - accuracy: 0.9781\n",
      "Epoch 413/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1164 - accuracy: 0.9722\n",
      "Epoch 414/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0707 - accuracy: 0.9812\n",
      "Epoch 415/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1633 - accuracy: 0.9606\n",
      "Epoch 416/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0786 - accuracy: 0.9805\n",
      "Epoch 417/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0753 - accuracy: 0.9802\n",
      "Epoch 418/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0884 - accuracy: 0.9773\n",
      "Epoch 419/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0759 - accuracy: 0.9803\n",
      "Epoch 420/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0956 - accuracy: 0.9763\n",
      "Epoch 421/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0745 - accuracy: 0.9801\n",
      "Epoch 422/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0813 - accuracy: 0.9792\n",
      "Epoch 423/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0770 - accuracy: 0.9798\n",
      "Epoch 424/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0831 - accuracy: 0.9777\n",
      "Epoch 425/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0938 - accuracy: 0.9764\n",
      "Epoch 426/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0641 - accuracy: 0.9831\n",
      "Epoch 427/500\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1564 - accuracy: 0.9613\n",
      "Epoch 428/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0686 - accuracy: 0.9811\n",
      "Epoch 429/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0726 - accuracy: 0.9814\n",
      "Epoch 430/500\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 0.0953 - accuracy: 0.9758\n",
      "Epoch 431/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.1039 - accuracy: 0.9758\n",
      "Epoch 432/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0834 - accuracy: 0.9782\n",
      "Epoch 433/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0907 - accuracy: 0.9760\n",
      "Epoch 434/500\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 0.0784 - accuracy: 0.9799\n",
      "Epoch 435/500\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 0.0735 - accuracy: 0.9807\n",
      "Epoch 436/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0745 - accuracy: 0.9804\n",
      "Epoch 437/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0857 - accuracy: 0.9777\n",
      "Epoch 438/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0969 - accuracy: 0.9756\n",
      "Epoch 439/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.1305 - accuracy: 0.9679\n",
      "Epoch 440/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0712 - accuracy: 0.9823\n",
      "Epoch 441/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0818 - accuracy: 0.9797\n",
      "Epoch 442/500\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.0865 - accuracy: 0.9782\n",
      "Epoch 443/500\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0629 - accuracy: 0.9834\n",
      "Epoch 444/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0960 - accuracy: 0.9774\n",
      "Epoch 445/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0711 - accuracy: 0.9819\n",
      "Epoch 446/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0879 - accuracy: 0.9783\n",
      "Epoch 447/500\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1022 - accuracy: 0.9754\n",
      "Epoch 448/500\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0714 - accuracy: 0.9821\n",
      "Epoch 449/500\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.0856 - accuracy: 0.9776\n",
      "Epoch 450/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0771 - accuracy: 0.9800\n",
      "Epoch 451/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.1046 - accuracy: 0.9729\n",
      "Epoch 452/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0878 - accuracy: 0.9778\n",
      "Epoch 453/500\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0986 - accuracy: 0.9796\n",
      "Epoch 454/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0811 - accuracy: 0.9796\n",
      "Epoch 455/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0638 - accuracy: 0.9826\n",
      "Epoch 456/500\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0802 - accuracy: 0.9794\n",
      "Epoch 457/500\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0891 - accuracy: 0.9792\n",
      "Epoch 458/500\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.0824 - accuracy: 0.9788\n",
      "Epoch 459/500\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.1060 - accuracy: 0.9730\n",
      "Epoch 460/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0815 - accuracy: 0.9788\n",
      "Epoch 461/500\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0758 - accuracy: 0.9808\n",
      "Epoch 462/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0693 - accuracy: 0.9816\n",
      "Epoch 463/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0893 - accuracy: 0.9767\n",
      "Epoch 464/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1128 - accuracy: 0.9713\n",
      "Epoch 465/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0616 - accuracy: 0.9843\n",
      "Epoch 466/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0716 - accuracy: 0.9814\n",
      "Epoch 467/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0752 - accuracy: 0.9805\n",
      "Epoch 468/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0760 - accuracy: 0.9813\n",
      "Epoch 469/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0981 - accuracy: 0.9752\n",
      "Epoch 470/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0854 - accuracy: 0.9802\n",
      "Epoch 471/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0716 - accuracy: 0.9817\n",
      "Epoch 472/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0785 - accuracy: 0.9805\n",
      "Epoch 473/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0773 - accuracy: 0.9807\n",
      "Epoch 474/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1330 - accuracy: 0.9710\n",
      "Epoch 475/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0793 - accuracy: 0.9798\n",
      "Epoch 476/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0798 - accuracy: 0.9808\n",
      "Epoch 477/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1206 - accuracy: 0.9713\n",
      "Epoch 478/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0641 - accuracy: 0.9839\n",
      "Epoch 479/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0756 - accuracy: 0.9808\n",
      "Epoch 480/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0854 - accuracy: 0.9800\n",
      "Epoch 481/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0801 - accuracy: 0.9807\n",
      "Epoch 482/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1290 - accuracy: 0.9714\n",
      "Epoch 483/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1030 - accuracy: 0.9748\n",
      "Epoch 484/500\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0612 - accuracy: 0.9842\n",
      "Epoch 485/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0670 - accuracy: 0.9827\n",
      "Epoch 486/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1200 - accuracy: 0.9722\n",
      "Epoch 487/500\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.0968 - accuracy: 0.9758\n",
      "Epoch 488/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0904 - accuracy: 0.9777\n",
      "Epoch 489/500\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0704 - accuracy: 0.9826\n",
      "Epoch 490/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0828 - accuracy: 0.9803\n",
      "Epoch 491/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0837 - accuracy: 0.9791\n",
      "Epoch 492/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1234 - accuracy: 0.9711\n",
      "Epoch 493/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0727 - accuracy: 0.9810\n",
      "Epoch 494/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0945 - accuracy: 0.9783\n",
      "Epoch 495/500\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0960 - accuracy: 0.9776\n",
      "Epoch 496/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0649 - accuracy: 0.9831\n",
      "Epoch 497/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0989 - accuracy: 0.9760\n",
      "Epoch 498/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1319 - accuracy: 0.9680\n",
      "Epoch 499/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0904 - accuracy: 0.9829\n",
      "Epoch 500/500\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.2910 - accuracy: 0.9300\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 5.1418 - accuracy: 0.5084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.141814231872559, 0.508400022983551]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn=simple_nn()\n",
    "simple_nn.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "simple_nn.summary()\n",
    "simple_nn.fit(x_train, y_train,epochs=500, shuffle=True)\n",
    "simple_nn.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u3zRHCushPjp",
   "metadata": {
    "id": "u3zRHCushPjp"
   },
   "source": [
    "### Results of Full conntected NN for 200 iterations 25 minutes,sgd opimization,relu activation,loss=categorical_crossentropy\n",
    "Train accuracy 1.00,loss: 8.0885e-05\\\n",
    "Test accuracy: 0.5219,loss: 6.9161"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zBkf2leHSy2g",
   "metadata": {
    "id": "zBkf2leHSy2g"
   },
   "source": [
    "### Results of Full conntected NN for 700 iterations,batch size=400, 46 minutes,sgd opimization,relu activation,loss=categorical_crossentropy\n",
    "Train accuracy: 1.0000,loss: 5.0130e-04 \\\n",
    "Test accuracy: 0.4819,loss: 6.4191 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t24HaKVel3cs",
   "metadata": {
    "id": "t24HaKVel3cs"
   },
   "source": [
    "### Results of Full conntected NN for 900 iterations, 2 hours 23 minutes,sgd opimization,relu activation,loss=categorical_crossentropy\n",
    "Train  accuracy: 1.0000,loss: 8.5791e-06 \\\n",
    "Test accuracy: 0.5139,loss: 8.4879 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8FMHScpUvAk6",
   "metadata": {
    "id": "8FMHScpUvAk6"
   },
   "source": [
    "### Results of Full conntected NN for 500 iterations 2 hours 20 minutes,adam opimization,relu activation,loss=categorical_crossentropy\n",
    "Train accuracy: 0.9757,loss: 0.0865 \\\n",
    "Test accuracy: 0.5084,loss: 5.1418 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ErhkXh7nLbwV",
   "metadata": {
    "id": "ErhkXh7nLbwV"
   },
   "source": [
    "### Results of Full conntected NN for 200 iterations, 30 minutes training,adam opimization,relu activation,loss=categorical_crossentropy\n",
    "Train accuracy: 0.9622,loss:0.1230 \\\n",
    "Test accuracy: 0.5061,loss: 4.7828"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WU2hgDDl8Agz",
   "metadata": {
    "id": "WU2hgDDl8Agz"
   },
   "source": [
    "### Results of Full conntected NN for batch size=400,700 iterations,48 minutes,adam opimization,sigmoid activation,loss=categorical_crossentropy\n",
    " Train accuracy: 0.9891,loss: 0.0332 \\\n",
    " Test accuracy: 0.4195,loss: 5.3026 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lTXJvcktybjN",
   "metadata": {
    "id": "lTXJvcktybjN"
   },
   "source": [
    "### Results of Full conntected NN for 500 iterations 1 hour 47 minutes training,adam opimization,tanh activation,loss=categorical_crossentropy\n",
    "Train accuracy: 0.5865,loss: 1.1654 \\\n",
    "Test accuracy: 0.4466,loss: 1.6616"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OzvFzAgfmrtH",
   "metadata": {
    "id": "OzvFzAgfmrtH"
   },
   "source": [
    "### Evaluation:\n",
    "Sgd optimiazation and relu gets worse with more iterations.Its  worst with sgd optimization than adam, after more iterations\n",
    "relu is better activation than sigmoid and tanh is the worst.Its perfomance is much better and faster than similar NN from scratch.With adam and more iterations train accuracy improves and slightly improves test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SusbhK7fnLvw",
   "metadata": {
    "id": "SusbhK7fnLvw"
   },
   "source": [
    "### Keras Implementation of LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wQPVR2SZC0n_",
   "metadata": {
    "id": "wQPVR2SZC0n_"
   },
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    \"\"\"\n",
    "    Implements the  model:\n",
    "    CONV2D ->  MAXPOOL -> CONV2D  -> MAXPOOL  -> DENSE -> DENSE -> SOFTMAX\n",
    "     \n",
    "    Arguments:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    model= Keras Sequential model \n",
    "    \"\"\"\n",
    "    model =Sequential()\n",
    "    #Conv layer 1 with 8 5x5 filters and stride of 1 and sigmoid activation       \n",
    "    model.add(Conv2D(filters = 8,kernel_size=5,strides=1,activation ='relu',input_shape = (32,32,3)))\n",
    "    # Max Pooling layer 1 with pool size =2 and stride of 2\n",
    "    model.add(MaxPooling2D(pool_size =2,strides=2))\n",
    "    #Conv layer 2 with 16 5x5 filters and stride of 1 and sigmoid activation \n",
    "    model.add(Conv2D(filters = 16,kernel_size=5,strides=1,activation ='relu',input_shape = (14,14,3)))\n",
    "    # Max Pooling layer 2 with pool size =2 and stride of 2\n",
    "    model.add(MaxPooling2D(pool_size =2,strides=2))\n",
    "    #Flatten\n",
    "    model.add(Flatten())\n",
    "    #Fully Connected Dense layer 1 with 120 unit for output & sigmoid activation\n",
    "    model.add(Dense(units=120,activation='relu'))\n",
    "    #Fully Connected Dense layer 2 with 84 unit for output & sigmoid activation\n",
    "    model.add(Dense(units=84,activation='relu'))\n",
    "    #Output Layer\n",
    "    model.add(Dense(units=10,activation='softmax'))  \n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mHHb7UD6nH-l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHHb7UD6nH-l",
    "outputId": "ad6dd72d-ee2e-4840-845f-994f60341565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 8)         608       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 16)        3216      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62,958\n",
      "Trainable params: 62,958\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.7396 - accuracy: 0.3626\n",
      "Epoch 2/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.4376 - accuracy: 0.4791\n",
      "Epoch 3/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3265 - accuracy: 0.5229\n",
      "Epoch 4/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.2456 - accuracy: 0.5559\n",
      "Epoch 5/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.1870 - accuracy: 0.5792\n",
      "Epoch 6/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.1363 - accuracy: 0.5996\n",
      "Epoch 7/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.0935 - accuracy: 0.6137\n",
      "Epoch 8/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.0553 - accuracy: 0.6270\n",
      "Epoch 9/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.0187 - accuracy: 0.6416\n",
      "Epoch 10/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.9862 - accuracy: 0.6505\n",
      "Epoch 11/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.9556 - accuracy: 0.6664\n",
      "Epoch 12/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.9316 - accuracy: 0.6710\n",
      "Epoch 13/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.9026 - accuracy: 0.6831\n",
      "Epoch 14/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.8767 - accuracy: 0.6924\n",
      "Epoch 15/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.8512 - accuracy: 0.7025\n",
      "Epoch 16/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.8294 - accuracy: 0.7108\n",
      "Epoch 17/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.8100 - accuracy: 0.7158\n",
      "Epoch 18/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.7879 - accuracy: 0.7255\n",
      "Epoch 19/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.7657 - accuracy: 0.7313\n",
      "Epoch 20/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.7526 - accuracy: 0.7362\n",
      "Epoch 21/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.7302 - accuracy: 0.7429\n",
      "Epoch 22/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.7143 - accuracy: 0.7499\n",
      "Epoch 23/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.6964 - accuracy: 0.7572\n",
      "Epoch 24/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.6784 - accuracy: 0.7614\n",
      "Epoch 25/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.6641 - accuracy: 0.7667\n",
      "Epoch 26/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.6457 - accuracy: 0.7726\n",
      "Epoch 27/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.6325 - accuracy: 0.7780\n",
      "Epoch 28/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.6170 - accuracy: 0.7826\n",
      "Epoch 29/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.6020 - accuracy: 0.7884\n",
      "Epoch 30/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.5874 - accuracy: 0.7931\n",
      "Epoch 31/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.5765 - accuracy: 0.7974\n",
      "Epoch 32/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.5603 - accuracy: 0.8041\n",
      "Epoch 33/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.5462 - accuracy: 0.8065\n",
      "Epoch 34/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.5333 - accuracy: 0.8134\n",
      "Epoch 35/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.5228 - accuracy: 0.8126\n",
      "Epoch 36/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.5077 - accuracy: 0.8203\n",
      "Epoch 37/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.4962 - accuracy: 0.8242\n",
      "Epoch 38/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.4828 - accuracy: 0.8280\n",
      "Epoch 39/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.4718 - accuracy: 0.8327\n",
      "Epoch 40/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.4632 - accuracy: 0.8344\n",
      "Epoch 41/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.4550 - accuracy: 0.8359\n",
      "Epoch 42/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.4397 - accuracy: 0.8429\n",
      "Epoch 43/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.4316 - accuracy: 0.8455\n",
      "Epoch 44/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.4215 - accuracy: 0.8494\n",
      "Epoch 45/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.4108 - accuracy: 0.8526\n",
      "Epoch 46/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3992 - accuracy: 0.8560\n",
      "Epoch 47/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.3930 - accuracy: 0.8596\n",
      "Epoch 48/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.3837 - accuracy: 0.8607\n",
      "Epoch 49/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3715 - accuracy: 0.8654\n",
      "Epoch 50/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3692 - accuracy: 0.8671\n",
      "Epoch 51/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3582 - accuracy: 0.8710\n",
      "Epoch 52/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3493 - accuracy: 0.8732\n",
      "Epoch 53/700\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.3400 - accuracy: 0.8773\n",
      "Epoch 54/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3381 - accuracy: 0.8775\n",
      "Epoch 55/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3308 - accuracy: 0.8784\n",
      "Epoch 56/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3214 - accuracy: 0.8829\n",
      "Epoch 57/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3148 - accuracy: 0.8847\n",
      "Epoch 58/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3074 - accuracy: 0.8867\n",
      "Epoch 59/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2980 - accuracy: 0.8924\n",
      "Epoch 60/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2970 - accuracy: 0.8918\n",
      "Epoch 61/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2938 - accuracy: 0.8928\n",
      "Epoch 62/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2883 - accuracy: 0.8951\n",
      "Epoch 63/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2814 - accuracy: 0.8975\n",
      "Epoch 64/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2726 - accuracy: 0.9017\n",
      "Epoch 65/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.2687 - accuracy: 0.9036\n",
      "Epoch 66/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2694 - accuracy: 0.9023\n",
      "Epoch 67/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2631 - accuracy: 0.9058\n",
      "Epoch 68/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2492 - accuracy: 0.9093\n",
      "Epoch 69/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2582 - accuracy: 0.9051\n",
      "Epoch 70/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2640 - accuracy: 0.9047\n",
      "Epoch 71/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2342 - accuracy: 0.9148\n",
      "Epoch 72/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2392 - accuracy: 0.9136\n",
      "Epoch 73/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2413 - accuracy: 0.9111\n",
      "Epoch 74/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2372 - accuracy: 0.9138\n",
      "Epoch 75/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2261 - accuracy: 0.9173\n",
      "Epoch 76/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.2270 - accuracy: 0.9163\n",
      "Epoch 77/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2195 - accuracy: 0.9209\n",
      "Epoch 78/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2273 - accuracy: 0.9171\n",
      "Epoch 79/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2237 - accuracy: 0.9185\n",
      "Epoch 80/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2237 - accuracy: 0.9192\n",
      "Epoch 81/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.2151 - accuracy: 0.9221\n",
      "Epoch 82/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2105 - accuracy: 0.9250\n",
      "Epoch 83/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2072 - accuracy: 0.9243\n",
      "Epoch 84/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2022 - accuracy: 0.9280\n",
      "Epoch 85/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1884 - accuracy: 0.9323\n",
      "Epoch 86/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1896 - accuracy: 0.9318\n",
      "Epoch 87/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1995 - accuracy: 0.9289\n",
      "Epoch 88/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2131 - accuracy: 0.9244\n",
      "Epoch 89/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1903 - accuracy: 0.9314\n",
      "Epoch 90/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1936 - accuracy: 0.9306\n",
      "Epoch 91/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1761 - accuracy: 0.9369\n",
      "Epoch 92/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1813 - accuracy: 0.9352\n",
      "Epoch 93/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1981 - accuracy: 0.9284\n",
      "Epoch 94/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1940 - accuracy: 0.9309\n",
      "Epoch 95/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1769 - accuracy: 0.9368\n",
      "Epoch 96/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1833 - accuracy: 0.9350\n",
      "Epoch 97/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1885 - accuracy: 0.9330\n",
      "Epoch 98/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1781 - accuracy: 0.9368\n",
      "Epoch 99/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1789 - accuracy: 0.9365\n",
      "Epoch 100/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1806 - accuracy: 0.9356\n",
      "Epoch 101/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1805 - accuracy: 0.9355\n",
      "Epoch 102/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1786 - accuracy: 0.9372\n",
      "Epoch 103/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1820 - accuracy: 0.9370\n",
      "Epoch 104/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1611 - accuracy: 0.9427\n",
      "Epoch 105/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1583 - accuracy: 0.9449\n",
      "Epoch 106/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1429 - accuracy: 0.9491\n",
      "Epoch 107/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1632 - accuracy: 0.9429\n",
      "Epoch 108/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1772 - accuracy: 0.9375\n",
      "Epoch 109/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1559 - accuracy: 0.9460\n",
      "Epoch 110/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1552 - accuracy: 0.9459\n",
      "Epoch 111/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1756 - accuracy: 0.9386\n",
      "Epoch 112/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1531 - accuracy: 0.9462\n",
      "Epoch 113/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1540 - accuracy: 0.9463\n",
      "Epoch 114/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1648 - accuracy: 0.9432\n",
      "Epoch 115/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1417 - accuracy: 0.9501\n",
      "Epoch 116/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1607 - accuracy: 0.9440\n",
      "Epoch 117/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1626 - accuracy: 0.9452\n",
      "Epoch 118/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1508 - accuracy: 0.9478\n",
      "Epoch 119/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1507 - accuracy: 0.9468\n",
      "Epoch 120/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1456 - accuracy: 0.9502\n",
      "Epoch 121/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1885 - accuracy: 0.9376\n",
      "Epoch 122/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1554 - accuracy: 0.9460\n",
      "Epoch 123/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1235 - accuracy: 0.9565\n",
      "Epoch 124/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1571 - accuracy: 0.9472\n",
      "Epoch 125/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1499 - accuracy: 0.9496\n",
      "Epoch 126/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1185 - accuracy: 0.9592\n",
      "Epoch 127/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1650 - accuracy: 0.9449\n",
      "Epoch 128/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1382 - accuracy: 0.9532\n",
      "Epoch 129/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1332 - accuracy: 0.9540\n",
      "Epoch 130/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1425 - accuracy: 0.9518\n",
      "Epoch 131/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1601 - accuracy: 0.9474\n",
      "Epoch 132/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1531 - accuracy: 0.9487\n",
      "Epoch 133/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1374 - accuracy: 0.9537\n",
      "Epoch 134/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1504 - accuracy: 0.9498\n",
      "Epoch 135/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1407 - accuracy: 0.9520\n",
      "Epoch 136/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1132 - accuracy: 0.9597\n",
      "Epoch 137/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1347 - accuracy: 0.9544\n",
      "Epoch 138/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1458 - accuracy: 0.9508\n",
      "Epoch 139/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1428 - accuracy: 0.9514\n",
      "Epoch 140/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1466 - accuracy: 0.9519\n",
      "Epoch 141/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1391 - accuracy: 0.9539\n",
      "Epoch 142/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1449 - accuracy: 0.9503\n",
      "Epoch 143/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1239 - accuracy: 0.9586\n",
      "Epoch 144/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1354 - accuracy: 0.9559\n",
      "Epoch 145/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1206 - accuracy: 0.9598\n",
      "Epoch 146/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1556 - accuracy: 0.9488\n",
      "Epoch 147/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1815 - accuracy: 0.9413\n",
      "Epoch 148/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1467 - accuracy: 0.9517\n",
      "Epoch 149/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1280 - accuracy: 0.9573\n",
      "Epoch 150/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1096 - accuracy: 0.9634\n",
      "Epoch 151/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1272 - accuracy: 0.9580\n",
      "Epoch 152/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1467 - accuracy: 0.9521\n",
      "Epoch 153/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1337 - accuracy: 0.9566\n",
      "Epoch 154/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.0875 - accuracy: 0.9709\n",
      "Epoch 155/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1155 - accuracy: 0.9610\n",
      "Epoch 156/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1505 - accuracy: 0.9520\n",
      "Epoch 157/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1702 - accuracy: 0.9457\n",
      "Epoch 158/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1288 - accuracy: 0.9569\n",
      "Epoch 159/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1217 - accuracy: 0.9608\n",
      "Epoch 160/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1335 - accuracy: 0.9575\n",
      "Epoch 161/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1220 - accuracy: 0.9598\n",
      "Epoch 162/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1500 - accuracy: 0.9531\n",
      "Epoch 163/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1654 - accuracy: 0.9464\n",
      "Epoch 164/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1484 - accuracy: 0.9536\n",
      "Epoch 165/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1089 - accuracy: 0.9635\n",
      "Epoch 166/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.0917 - accuracy: 0.9684\n",
      "Epoch 167/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1250 - accuracy: 0.9606\n",
      "Epoch 168/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1230 - accuracy: 0.9597\n",
      "Epoch 169/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1629 - accuracy: 0.9486\n",
      "Epoch 170/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1459 - accuracy: 0.9515\n",
      "Epoch 171/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1397 - accuracy: 0.9561\n",
      "Epoch 172/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1664 - accuracy: 0.9495\n",
      "Epoch 173/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1223 - accuracy: 0.9604\n",
      "Epoch 174/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.0934 - accuracy: 0.9687\n",
      "Epoch 175/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.0903 - accuracy: 0.9695\n",
      "Epoch 176/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.0967 - accuracy: 0.9676\n",
      "Epoch 177/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1446 - accuracy: 0.9560\n",
      "Epoch 178/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1196 - accuracy: 0.9614\n",
      "Epoch 179/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1355 - accuracy: 0.9578\n",
      "Epoch 180/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1512 - accuracy: 0.9526\n",
      "Epoch 181/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1244 - accuracy: 0.9611\n",
      "Epoch 182/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1046 - accuracy: 0.9663\n",
      "Epoch 183/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1071 - accuracy: 0.9644\n",
      "Epoch 184/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1202 - accuracy: 0.9622\n",
      "Epoch 185/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1325 - accuracy: 0.9579\n",
      "Epoch 186/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1351 - accuracy: 0.9577\n",
      "Epoch 187/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1357 - accuracy: 0.9570\n",
      "Epoch 188/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1423 - accuracy: 0.9545\n",
      "Epoch 189/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1253 - accuracy: 0.9606\n",
      "Epoch 190/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1039 - accuracy: 0.9667\n",
      "Epoch 191/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1195 - accuracy: 0.9633\n",
      "Epoch 192/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1575 - accuracy: 0.9522\n",
      "Epoch 193/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1476 - accuracy: 0.9543\n",
      "Epoch 194/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1391 - accuracy: 0.9570\n",
      "Epoch 195/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1358 - accuracy: 0.9584\n",
      "Epoch 196/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1186 - accuracy: 0.9630\n",
      "Epoch 197/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1209 - accuracy: 0.9620\n",
      "Epoch 198/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1174 - accuracy: 0.9636\n",
      "Epoch 199/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1437 - accuracy: 0.9560\n",
      "Epoch 200/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.0960 - accuracy: 0.9688\n",
      "Epoch 201/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.0796 - accuracy: 0.9739\n",
      "Epoch 202/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1277 - accuracy: 0.9611\n",
      "Epoch 203/700\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1513 - accuracy: 0.9544\n",
      "Epoch 204/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1358 - accuracy: 0.9585\n",
      "Epoch 205/700\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 0.1214 - accuracy: 0.9632\n",
      "Epoch 206/700\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 0.1281 - accuracy: 0.9607\n",
      "Epoch 207/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1201 - accuracy: 0.9625\n",
      "Epoch 208/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1194 - accuracy: 0.9652\n",
      "Epoch 209/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1254 - accuracy: 0.9608\n",
      "Epoch 210/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1401 - accuracy: 0.9567\n",
      "Epoch 211/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1354 - accuracy: 0.9586\n",
      "Epoch 212/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1259 - accuracy: 0.9614\n",
      "Epoch 213/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1419 - accuracy: 0.9581\n",
      "Epoch 214/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1570 - accuracy: 0.9529\n",
      "Epoch 215/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1417 - accuracy: 0.9569\n",
      "Epoch 216/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1002 - accuracy: 0.9677\n",
      "Epoch 217/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1262 - accuracy: 0.9618\n",
      "Epoch 218/700\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 0.1316 - accuracy: 0.9599\n",
      "Epoch 219/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1036 - accuracy: 0.9664\n",
      "Epoch 220/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1092 - accuracy: 0.9660\n",
      "Epoch 221/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1368 - accuracy: 0.9594\n",
      "Epoch 222/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1398 - accuracy: 0.9575\n",
      "Epoch 223/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1309 - accuracy: 0.9608\n",
      "Epoch 224/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1793 - accuracy: 0.9475\n",
      "Epoch 225/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1413 - accuracy: 0.9585\n",
      "Epoch 226/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1260 - accuracy: 0.9609\n",
      "Epoch 227/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1070 - accuracy: 0.9660\n",
      "Epoch 228/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.0947 - accuracy: 0.9704\n",
      "Epoch 229/700\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1425 - accuracy: 0.9591\n",
      "Epoch 230/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1286 - accuracy: 0.9620\n",
      "Epoch 231/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1587 - accuracy: 0.9550\n",
      "Epoch 232/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1483 - accuracy: 0.9565\n",
      "Epoch 233/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1334 - accuracy: 0.9605\n",
      "Epoch 234/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1352 - accuracy: 0.9601\n",
      "Epoch 235/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1063 - accuracy: 0.9669\n",
      "Epoch 236/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.0980 - accuracy: 0.9704\n",
      "Epoch 237/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1108 - accuracy: 0.9658\n",
      "Epoch 238/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1089 - accuracy: 0.9677\n",
      "Epoch 239/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1475 - accuracy: 0.9586\n",
      "Epoch 240/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1772 - accuracy: 0.9495\n",
      "Epoch 241/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1412 - accuracy: 0.9579\n",
      "Epoch 242/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1131 - accuracy: 0.9658\n",
      "Epoch 243/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1200 - accuracy: 0.9639\n",
      "Epoch 244/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1381 - accuracy: 0.9591\n",
      "Epoch 245/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1185 - accuracy: 0.9637\n",
      "Epoch 246/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1070 - accuracy: 0.9674\n",
      "Epoch 247/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1109 - accuracy: 0.9672\n",
      "Epoch 248/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1284 - accuracy: 0.9628\n",
      "Epoch 249/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1349 - accuracy: 0.9607\n",
      "Epoch 250/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1538 - accuracy: 0.9576\n",
      "Epoch 251/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1254 - accuracy: 0.9621\n",
      "Epoch 252/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1240 - accuracy: 0.9635\n",
      "Epoch 253/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1293 - accuracy: 0.9632\n",
      "Epoch 254/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1695 - accuracy: 0.9524\n",
      "Epoch 255/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1299 - accuracy: 0.9617\n",
      "Epoch 256/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1147 - accuracy: 0.9662\n",
      "Epoch 257/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1225 - accuracy: 0.9649\n",
      "Epoch 258/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1236 - accuracy: 0.9636\n",
      "Epoch 259/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1330 - accuracy: 0.9615\n",
      "Epoch 260/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1199 - accuracy: 0.9641\n",
      "Epoch 261/700\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1292 - accuracy: 0.9630\n",
      "Epoch 262/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.0949 - accuracy: 0.9704\n",
      "Epoch 263/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1371 - accuracy: 0.9610\n",
      "Epoch 264/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1377 - accuracy: 0.9604\n",
      "Epoch 265/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1247 - accuracy: 0.9632\n",
      "Epoch 266/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1567 - accuracy: 0.9566\n",
      "Epoch 267/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1576 - accuracy: 0.9564\n",
      "Epoch 268/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1350 - accuracy: 0.9609\n",
      "Epoch 269/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1464 - accuracy: 0.9593\n",
      "Epoch 270/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1188 - accuracy: 0.9649\n",
      "Epoch 271/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1153 - accuracy: 0.9656\n",
      "Epoch 272/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1366 - accuracy: 0.9602\n",
      "Epoch 273/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1566 - accuracy: 0.9573\n",
      "Epoch 274/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1665 - accuracy: 0.9534\n",
      "Epoch 275/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1215 - accuracy: 0.9646\n",
      "Epoch 276/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1387 - accuracy: 0.9611\n",
      "Epoch 277/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1606 - accuracy: 0.9557\n",
      "Epoch 278/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1273 - accuracy: 0.9637\n",
      "Epoch 279/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1180 - accuracy: 0.9657\n",
      "Epoch 280/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1200 - accuracy: 0.9649\n",
      "Epoch 281/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1230 - accuracy: 0.9645\n",
      "Epoch 282/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1224 - accuracy: 0.9650\n",
      "Epoch 283/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1578 - accuracy: 0.9586\n",
      "Epoch 284/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1708 - accuracy: 0.9526\n",
      "Epoch 285/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1526 - accuracy: 0.9572\n",
      "Epoch 286/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1506 - accuracy: 0.9595\n",
      "Epoch 287/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1348 - accuracy: 0.9619\n",
      "Epoch 288/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1384 - accuracy: 0.9614\n",
      "Epoch 289/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1247 - accuracy: 0.9642\n",
      "Epoch 290/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1358 - accuracy: 0.9630\n",
      "Epoch 291/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1171 - accuracy: 0.9662\n",
      "Epoch 292/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1692 - accuracy: 0.9544\n",
      "Epoch 293/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1426 - accuracy: 0.9607\n",
      "Epoch 294/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1255 - accuracy: 0.9644\n",
      "Epoch 295/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1124 - accuracy: 0.9677\n",
      "Epoch 296/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1244 - accuracy: 0.9657\n",
      "Epoch 297/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1440 - accuracy: 0.9618\n",
      "Epoch 298/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1088 - accuracy: 0.9690\n",
      "Epoch 299/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1290 - accuracy: 0.9639\n",
      "Epoch 300/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1618 - accuracy: 0.9558\n",
      "Epoch 301/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1809 - accuracy: 0.9524\n",
      "Epoch 302/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1968 - accuracy: 0.9489\n",
      "Epoch 303/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1556 - accuracy: 0.9576\n",
      "Epoch 304/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1268 - accuracy: 0.9651\n",
      "Epoch 305/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1266 - accuracy: 0.9648\n",
      "Epoch 306/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1337 - accuracy: 0.9629\n",
      "Epoch 307/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1235 - accuracy: 0.9645\n",
      "Epoch 308/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1373 - accuracy: 0.9627\n",
      "Epoch 309/700\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 0.1418 - accuracy: 0.9614\n",
      "Epoch 310/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1487 - accuracy: 0.9599\n",
      "Epoch 311/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1519 - accuracy: 0.9597\n",
      "Epoch 312/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2022 - accuracy: 0.9483\n",
      "Epoch 313/700\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1471 - accuracy: 0.9582\n",
      "Epoch 314/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1516 - accuracy: 0.9602\n",
      "Epoch 315/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1345 - accuracy: 0.9627\n",
      "Epoch 316/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1441 - accuracy: 0.9614\n",
      "Epoch 317/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1428 - accuracy: 0.9607\n",
      "Epoch 318/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1405 - accuracy: 0.9616\n",
      "Epoch 319/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1588 - accuracy: 0.9573\n",
      "Epoch 320/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1259 - accuracy: 0.9649\n",
      "Epoch 321/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1387 - accuracy: 0.9627\n",
      "Epoch 322/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1314 - accuracy: 0.9629\n",
      "Epoch 323/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1402 - accuracy: 0.9616\n",
      "Epoch 324/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1494 - accuracy: 0.9602\n",
      "Epoch 325/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1781 - accuracy: 0.9532\n",
      "Epoch 326/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1618 - accuracy: 0.9568\n",
      "Epoch 327/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1496 - accuracy: 0.9604\n",
      "Epoch 328/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1456 - accuracy: 0.9611\n",
      "Epoch 329/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1405 - accuracy: 0.9616\n",
      "Epoch 330/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1183 - accuracy: 0.9669\n",
      "Epoch 331/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1441 - accuracy: 0.9614\n",
      "Epoch 332/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1269 - accuracy: 0.9644\n",
      "Epoch 333/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1337 - accuracy: 0.9629\n",
      "Epoch 334/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1530 - accuracy: 0.9592\n",
      "Epoch 335/700\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 0.1949 - accuracy: 0.9504\n",
      "Epoch 336/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1688 - accuracy: 0.9546\n",
      "Epoch 337/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1470 - accuracy: 0.9615\n",
      "Epoch 338/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1307 - accuracy: 0.9640\n",
      "Epoch 339/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1455 - accuracy: 0.9622\n",
      "Epoch 340/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1729 - accuracy: 0.9570\n",
      "Epoch 341/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1506 - accuracy: 0.9597\n",
      "Epoch 342/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1365 - accuracy: 0.9635\n",
      "Epoch 343/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1613 - accuracy: 0.9584\n",
      "Epoch 344/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1098 - accuracy: 0.9696\n",
      "Epoch 345/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1066 - accuracy: 0.9702\n",
      "Epoch 346/700\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1036 - accuracy: 0.9706\n",
      "Epoch 347/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1317 - accuracy: 0.9649\n",
      "Epoch 348/700\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1707 - accuracy: 0.9573\n",
      "Epoch 349/700\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2163 - accuracy: 0.9480\n",
      "Epoch 350/700\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1901 - accuracy: 0.9525\n",
      "Epoch 351/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1615 - accuracy: 0.9579\n",
      "Epoch 352/700\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1421 - accuracy: 0.9619\n",
      "Epoch 353/700\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1593 - accuracy: 0.9579\n",
      "Epoch 354/700\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1398 - accuracy: 0.9615\n",
      "Epoch 355/700\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1398 - accuracy: 0.9631\n",
      "Epoch 356/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1187 - accuracy: 0.9678\n",
      "Epoch 357/700\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1435 - accuracy: 0.9625\n",
      "Epoch 358/700\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1257 - accuracy: 0.9658\n",
      "Epoch 359/700\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1678 - accuracy: 0.9582\n",
      "Epoch 360/700\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1562 - accuracy: 0.9591\n",
      "Epoch 361/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1513 - accuracy: 0.9605\n",
      "Epoch 362/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1497 - accuracy: 0.9605\n",
      "Epoch 363/700\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1733 - accuracy: 0.9564\n",
      "Epoch 364/700\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1310 - accuracy: 0.9649\n",
      "Epoch 365/700\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1360 - accuracy: 0.9642\n",
      "Epoch 366/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1697 - accuracy: 0.9573\n",
      "Epoch 367/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1545 - accuracy: 0.9604\n",
      "Epoch 368/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1703 - accuracy: 0.9565\n",
      "Epoch 369/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1542 - accuracy: 0.9597\n",
      "Epoch 370/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1356 - accuracy: 0.9640\n",
      "Epoch 371/700\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1253 - accuracy: 0.9658\n",
      "Epoch 372/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1570 - accuracy: 0.9597\n",
      "Epoch 373/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1585 - accuracy: 0.9593\n",
      "Epoch 374/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1672 - accuracy: 0.9577\n",
      "Epoch 375/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1767 - accuracy: 0.9562\n",
      "Epoch 376/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1423 - accuracy: 0.9624\n",
      "Epoch 377/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1373 - accuracy: 0.9648\n",
      "Epoch 378/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1327 - accuracy: 0.9654\n",
      "Epoch 379/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1355 - accuracy: 0.9654\n",
      "Epoch 380/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1523 - accuracy: 0.9607\n",
      "Epoch 381/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1519 - accuracy: 0.9614\n",
      "Epoch 382/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1837 - accuracy: 0.9549\n",
      "Epoch 383/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1753 - accuracy: 0.9557\n",
      "Epoch 384/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1580 - accuracy: 0.9600\n",
      "Epoch 385/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1496 - accuracy: 0.9622\n",
      "Epoch 386/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1619 - accuracy: 0.9599\n",
      "Epoch 387/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1707 - accuracy: 0.9576\n",
      "Epoch 388/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1505 - accuracy: 0.9608\n",
      "Epoch 389/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1970 - accuracy: 0.9508\n",
      "Epoch 390/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1831 - accuracy: 0.9549\n",
      "Epoch 391/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1479 - accuracy: 0.9625\n",
      "Epoch 392/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1359 - accuracy: 0.9636\n",
      "Epoch 393/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1496 - accuracy: 0.9624\n",
      "Epoch 394/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1471 - accuracy: 0.9611\n",
      "Epoch 395/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1586 - accuracy: 0.9605\n",
      "Epoch 396/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1655 - accuracy: 0.9575\n",
      "Epoch 397/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1629 - accuracy: 0.9588\n",
      "Epoch 398/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1817 - accuracy: 0.9553\n",
      "Epoch 399/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1648 - accuracy: 0.9586\n",
      "Epoch 400/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1725 - accuracy: 0.9561\n",
      "Epoch 401/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1199 - accuracy: 0.9683\n",
      "Epoch 402/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1045 - accuracy: 0.9707\n",
      "Epoch 403/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1525 - accuracy: 0.9628\n",
      "Epoch 404/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1657 - accuracy: 0.9597\n",
      "Epoch 405/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2009 - accuracy: 0.9528\n",
      "Epoch 406/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1764 - accuracy: 0.9567\n",
      "Epoch 407/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1581 - accuracy: 0.9605\n",
      "Epoch 408/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1636 - accuracy: 0.9606\n",
      "Epoch 409/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1546 - accuracy: 0.9612\n",
      "Epoch 410/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2101 - accuracy: 0.9508\n",
      "Epoch 411/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1499 - accuracy: 0.9614\n",
      "Epoch 412/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1657 - accuracy: 0.9588\n",
      "Epoch 413/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1388 - accuracy: 0.9642\n",
      "Epoch 414/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1474 - accuracy: 0.9632\n",
      "Epoch 415/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1781 - accuracy: 0.9562\n",
      "Epoch 416/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1874 - accuracy: 0.9552\n",
      "Epoch 417/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1569 - accuracy: 0.9608\n",
      "Epoch 418/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1638 - accuracy: 0.9595\n",
      "Epoch 419/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1898 - accuracy: 0.9544\n",
      "Epoch 420/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1509 - accuracy: 0.9615\n",
      "Epoch 421/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1481 - accuracy: 0.9625\n",
      "Epoch 422/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1556 - accuracy: 0.9612\n",
      "Epoch 423/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1838 - accuracy: 0.9559\n",
      "Epoch 424/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1988 - accuracy: 0.9532\n",
      "Epoch 425/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1599 - accuracy: 0.9605\n",
      "Epoch 426/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1482 - accuracy: 0.9632\n",
      "Epoch 427/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1778 - accuracy: 0.9577\n",
      "Epoch 428/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1323 - accuracy: 0.9666\n",
      "Epoch 429/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1472 - accuracy: 0.9632\n",
      "Epoch 430/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1509 - accuracy: 0.9624\n",
      "Epoch 431/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1638 - accuracy: 0.9598\n",
      "Epoch 432/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2091 - accuracy: 0.9519\n",
      "Epoch 433/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1983 - accuracy: 0.9532\n",
      "Epoch 434/700\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.2032 - accuracy: 0.9525\n",
      "Epoch 435/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1767 - accuracy: 0.9566\n",
      "Epoch 436/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1158 - accuracy: 0.9697\n",
      "Epoch 437/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1205 - accuracy: 0.9698\n",
      "Epoch 438/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.2036 - accuracy: 0.9533\n",
      "Epoch 439/700\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1757 - accuracy: 0.9581\n",
      "Epoch 440/700\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1902 - accuracy: 0.9547\n",
      "Epoch 441/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1627 - accuracy: 0.9600\n",
      "Epoch 442/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1598 - accuracy: 0.9622\n",
      "Epoch 443/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.2000 - accuracy: 0.9538\n",
      "Epoch 444/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1919 - accuracy: 0.9543\n",
      "Epoch 445/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1357 - accuracy: 0.9655\n",
      "Epoch 446/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1361 - accuracy: 0.9661\n",
      "Epoch 447/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1507 - accuracy: 0.9645\n",
      "Epoch 448/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1597 - accuracy: 0.9608\n",
      "Epoch 449/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1572 - accuracy: 0.9617\n",
      "Epoch 450/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2113 - accuracy: 0.9526\n",
      "Epoch 451/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1650 - accuracy: 0.9599\n",
      "Epoch 452/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1668 - accuracy: 0.9587\n",
      "Epoch 453/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1675 - accuracy: 0.9591\n",
      "Epoch 454/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1856 - accuracy: 0.9562\n",
      "Epoch 455/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1908 - accuracy: 0.9555\n",
      "Epoch 456/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1412 - accuracy: 0.9649\n",
      "Epoch 457/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1476 - accuracy: 0.9644\n",
      "Epoch 458/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1740 - accuracy: 0.9572\n",
      "Epoch 459/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1730 - accuracy: 0.9594\n",
      "Epoch 460/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1452 - accuracy: 0.9639\n",
      "Epoch 461/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1611 - accuracy: 0.9617\n",
      "Epoch 462/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1953 - accuracy: 0.9550\n",
      "Epoch 463/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1786 - accuracy: 0.9583\n",
      "Epoch 464/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1397 - accuracy: 0.9654\n",
      "Epoch 465/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1665 - accuracy: 0.9612\n",
      "Epoch 466/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1473 - accuracy: 0.9639\n",
      "Epoch 467/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1870 - accuracy: 0.9554\n",
      "Epoch 468/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1663 - accuracy: 0.9596\n",
      "Epoch 469/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1649 - accuracy: 0.9604\n",
      "Epoch 470/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1475 - accuracy: 0.9642\n",
      "Epoch 471/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1515 - accuracy: 0.9628\n",
      "Epoch 472/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1703 - accuracy: 0.9601\n",
      "Epoch 473/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1935 - accuracy: 0.9557\n",
      "Epoch 474/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1978 - accuracy: 0.9559\n",
      "Epoch 475/700\n",
      "1563/1563 [==============================] - 34s 21ms/step - loss: 0.1544 - accuracy: 0.9623\n",
      "Epoch 476/700\n",
      "1563/1563 [==============================] - 34s 21ms/step - loss: 0.1521 - accuracy: 0.9623\n",
      "Epoch 477/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1712 - accuracy: 0.9600\n",
      "Epoch 478/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1785 - accuracy: 0.9570\n",
      "Epoch 479/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.2010 - accuracy: 0.9540\n",
      "Epoch 480/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1355 - accuracy: 0.9661\n",
      "Epoch 481/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1489 - accuracy: 0.9643\n",
      "Epoch 482/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1877 - accuracy: 0.9573\n",
      "Epoch 483/700\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1794 - accuracy: 0.9584\n",
      "Epoch 484/700\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1740 - accuracy: 0.9604\n",
      "Epoch 485/700\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1960 - accuracy: 0.9550\n",
      "Epoch 486/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.2120 - accuracy: 0.9505\n",
      "Epoch 487/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1631 - accuracy: 0.9610\n",
      "Epoch 488/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.2025 - accuracy: 0.9538\n",
      "Epoch 489/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1877 - accuracy: 0.9566\n",
      "Epoch 490/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1536 - accuracy: 0.9629\n",
      "Epoch 491/700\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1609 - accuracy: 0.9617\n",
      "Epoch 492/700\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1813 - accuracy: 0.9578\n",
      "Epoch 493/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2065 - accuracy: 0.9548\n",
      "Epoch 494/700\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1775 - accuracy: 0.9581\n",
      "Epoch 495/700\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1549 - accuracy: 0.9640\n",
      "Epoch 496/700\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1809 - accuracy: 0.9573\n",
      "Epoch 497/700\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1528 - accuracy: 0.9623\n",
      "Epoch 498/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1575 - accuracy: 0.9633\n",
      "Epoch 499/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1415 - accuracy: 0.9654\n",
      "Epoch 500/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1728 - accuracy: 0.9601\n",
      "Epoch 501/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.2241 - accuracy: 0.9531\n",
      "Epoch 502/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.2053 - accuracy: 0.9542\n",
      "Epoch 503/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1745 - accuracy: 0.9594\n",
      "Epoch 504/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1258 - accuracy: 0.9684\n",
      "Epoch 505/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1494 - accuracy: 0.9643\n",
      "Epoch 506/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2206 - accuracy: 0.9522\n",
      "Epoch 507/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1733 - accuracy: 0.9596\n",
      "Epoch 508/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1658 - accuracy: 0.9623\n",
      "Epoch 509/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1899 - accuracy: 0.9565\n",
      "Epoch 510/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1557 - accuracy: 0.9635\n",
      "Epoch 511/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1532 - accuracy: 0.9631\n",
      "Epoch 512/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1326 - accuracy: 0.9676\n",
      "Epoch 513/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1578 - accuracy: 0.9638\n",
      "Epoch 514/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.2025 - accuracy: 0.9554\n",
      "Epoch 515/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1669 - accuracy: 0.9618\n",
      "Epoch 516/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2021 - accuracy: 0.9559\n",
      "Epoch 517/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2037 - accuracy: 0.9561\n",
      "Epoch 518/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.2185 - accuracy: 0.9527\n",
      "Epoch 519/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1854 - accuracy: 0.9601\n",
      "Epoch 520/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1546 - accuracy: 0.9632\n",
      "Epoch 521/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1461 - accuracy: 0.9649\n",
      "Epoch 522/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1657 - accuracy: 0.9619\n",
      "Epoch 523/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1656 - accuracy: 0.9619\n",
      "Epoch 524/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1832 - accuracy: 0.9594\n",
      "Epoch 525/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.2483 - accuracy: 0.9479\n",
      "Epoch 526/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1936 - accuracy: 0.9558\n",
      "Epoch 527/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1576 - accuracy: 0.9641\n",
      "Epoch 528/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1733 - accuracy: 0.9590\n",
      "Epoch 529/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1325 - accuracy: 0.9685\n",
      "Epoch 530/700\n",
      "1563/1563 [==============================] - 34s 21ms/step - loss: 0.1735 - accuracy: 0.9595\n",
      "Epoch 531/700\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1818 - accuracy: 0.9599\n",
      "Epoch 532/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.2324 - accuracy: 0.9512\n",
      "Epoch 533/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1944 - accuracy: 0.9564\n",
      "Epoch 534/700\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1976 - accuracy: 0.9543\n",
      "Epoch 535/700\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1791 - accuracy: 0.9591\n",
      "Epoch 536/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.2083 - accuracy: 0.9556\n",
      "Epoch 537/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1888 - accuracy: 0.9575\n",
      "Epoch 538/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1292 - accuracy: 0.9686\n",
      "Epoch 539/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1482 - accuracy: 0.9662\n",
      "Epoch 540/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.2075 - accuracy: 0.9533\n",
      "Epoch 541/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.2148 - accuracy: 0.9535\n",
      "Epoch 542/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1538 - accuracy: 0.9637\n",
      "Epoch 543/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1954 - accuracy: 0.9568\n",
      "Epoch 544/700\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1586 - accuracy: 0.9630\n",
      "Epoch 545/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1529 - accuracy: 0.9644\n",
      "Epoch 546/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1755 - accuracy: 0.9593\n",
      "Epoch 547/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.2008 - accuracy: 0.9578\n",
      "Epoch 548/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1838 - accuracy: 0.9583\n",
      "Epoch 549/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1595 - accuracy: 0.9625\n",
      "Epoch 550/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1665 - accuracy: 0.9620\n",
      "Epoch 551/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1842 - accuracy: 0.9589\n",
      "Epoch 552/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1930 - accuracy: 0.9575\n",
      "Epoch 553/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1559 - accuracy: 0.9638\n",
      "Epoch 554/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1811 - accuracy: 0.9606\n",
      "Epoch 555/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1660 - accuracy: 0.9608\n",
      "Epoch 556/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1862 - accuracy: 0.9590\n",
      "Epoch 557/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1744 - accuracy: 0.9604\n",
      "Epoch 558/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1800 - accuracy: 0.9605\n",
      "Epoch 559/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1793 - accuracy: 0.9597\n",
      "Epoch 560/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1989 - accuracy: 0.9574\n",
      "Epoch 561/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1787 - accuracy: 0.9595\n",
      "Epoch 562/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1998 - accuracy: 0.9562\n",
      "Epoch 563/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1699 - accuracy: 0.9618\n",
      "Epoch 564/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1847 - accuracy: 0.9598\n",
      "Epoch 565/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1454 - accuracy: 0.9670\n",
      "Epoch 566/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1139 - accuracy: 0.9711\n",
      "Epoch 567/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1871 - accuracy: 0.9598\n",
      "Epoch 568/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1756 - accuracy: 0.9608\n",
      "Epoch 569/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2118 - accuracy: 0.9551\n",
      "Epoch 570/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2252 - accuracy: 0.9531\n",
      "Epoch 571/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.2300 - accuracy: 0.9511\n",
      "Epoch 572/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.2459 - accuracy: 0.9477\n",
      "Epoch 573/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1666 - accuracy: 0.9608\n",
      "Epoch 574/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1513 - accuracy: 0.9653\n",
      "Epoch 575/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1786 - accuracy: 0.9603\n",
      "Epoch 576/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.2097 - accuracy: 0.9544\n",
      "Epoch 577/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1976 - accuracy: 0.9570\n",
      "Epoch 578/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1751 - accuracy: 0.9605\n",
      "Epoch 579/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1836 - accuracy: 0.9603\n",
      "Epoch 580/700\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1715 - accuracy: 0.9628\n",
      "Epoch 581/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1861 - accuracy: 0.9596\n",
      "Epoch 582/700\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.1969 - accuracy: 0.9581\n",
      "Epoch 583/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1572 - accuracy: 0.9643\n",
      "Epoch 584/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1631 - accuracy: 0.9625\n",
      "Epoch 585/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.2361 - accuracy: 0.9536\n",
      "Epoch 586/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2267 - accuracy: 0.9527\n",
      "Epoch 587/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1862 - accuracy: 0.9581\n",
      "Epoch 588/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1781 - accuracy: 0.9606\n",
      "Epoch 589/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2063 - accuracy: 0.9545\n",
      "Epoch 590/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1638 - accuracy: 0.9630\n",
      "Epoch 591/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2156 - accuracy: 0.9534\n",
      "Epoch 592/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2095 - accuracy: 0.9557\n",
      "Epoch 593/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1852 - accuracy: 0.9575\n",
      "Epoch 594/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1810 - accuracy: 0.9611\n",
      "Epoch 595/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1702 - accuracy: 0.9620\n",
      "Epoch 596/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1132 - accuracy: 0.9725\n",
      "Epoch 597/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1671 - accuracy: 0.9627\n",
      "Epoch 598/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1695 - accuracy: 0.9614\n",
      "Epoch 599/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1762 - accuracy: 0.9613\n",
      "Epoch 600/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2185 - accuracy: 0.9562\n",
      "Epoch 601/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1763 - accuracy: 0.9614\n",
      "Epoch 602/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2077 - accuracy: 0.9561\n",
      "Epoch 603/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1950 - accuracy: 0.9569\n",
      "Epoch 604/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2424 - accuracy: 0.9511\n",
      "Epoch 605/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2296 - accuracy: 0.9518\n",
      "Epoch 606/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1821 - accuracy: 0.9592\n",
      "Epoch 607/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2301 - accuracy: 0.9531\n",
      "Epoch 608/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2180 - accuracy: 0.9531\n",
      "Epoch 609/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2057 - accuracy: 0.9552\n",
      "Epoch 610/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1795 - accuracy: 0.9609\n",
      "Epoch 611/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1397 - accuracy: 0.9669\n",
      "Epoch 612/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1897 - accuracy: 0.9585\n",
      "Epoch 613/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2496 - accuracy: 0.9518\n",
      "Epoch 614/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2149 - accuracy: 0.9536\n",
      "Epoch 615/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2012 - accuracy: 0.9568\n",
      "Epoch 616/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2029 - accuracy: 0.9572\n",
      "Epoch 617/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2025 - accuracy: 0.9571\n",
      "Epoch 618/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1779 - accuracy: 0.9613\n",
      "Epoch 619/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1716 - accuracy: 0.9620\n",
      "Epoch 620/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1402 - accuracy: 0.9676\n",
      "Epoch 621/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2156 - accuracy: 0.9555\n",
      "Epoch 622/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2034 - accuracy: 0.9564\n",
      "Epoch 623/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1994 - accuracy: 0.9573\n",
      "Epoch 624/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1802 - accuracy: 0.9615\n",
      "Epoch 625/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2123 - accuracy: 0.9553\n",
      "Epoch 626/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2503 - accuracy: 0.9498\n",
      "Epoch 627/700\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.2090 - accuracy: 0.9552\n",
      "Epoch 628/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1553 - accuracy: 0.9648\n",
      "Epoch 629/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2112 - accuracy: 0.9553\n",
      "Epoch 630/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1783 - accuracy: 0.9611\n",
      "Epoch 631/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1899 - accuracy: 0.9589\n",
      "Epoch 632/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1889 - accuracy: 0.9591\n",
      "Epoch 633/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1655 - accuracy: 0.9638\n",
      "Epoch 634/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2109 - accuracy: 0.9568\n",
      "Epoch 635/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2376 - accuracy: 0.9526\n",
      "Epoch 636/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2285 - accuracy: 0.9527\n",
      "Epoch 637/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2176 - accuracy: 0.9534\n",
      "Epoch 638/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2278 - accuracy: 0.9540\n",
      "Epoch 639/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1850 - accuracy: 0.9603\n",
      "Epoch 640/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1876 - accuracy: 0.9598\n",
      "Epoch 641/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1808 - accuracy: 0.9610\n",
      "Epoch 642/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1963 - accuracy: 0.9584\n",
      "Epoch 643/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1847 - accuracy: 0.9591\n",
      "Epoch 644/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1988 - accuracy: 0.9575\n",
      "Epoch 645/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1922 - accuracy: 0.9597\n",
      "Epoch 646/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2568 - accuracy: 0.9493\n",
      "Epoch 647/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1850 - accuracy: 0.9585\n",
      "Epoch 648/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1820 - accuracy: 0.9608\n",
      "Epoch 649/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1935 - accuracy: 0.9579\n",
      "Epoch 650/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2373 - accuracy: 0.9533\n",
      "Epoch 651/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1909 - accuracy: 0.9576\n",
      "Epoch 652/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1699 - accuracy: 0.9622\n",
      "Epoch 653/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1912 - accuracy: 0.9602\n",
      "Epoch 654/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1624 - accuracy: 0.9640\n",
      "Epoch 655/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1719 - accuracy: 0.9625\n",
      "Epoch 656/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1898 - accuracy: 0.9601\n",
      "Epoch 657/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2407 - accuracy: 0.9527\n",
      "Epoch 658/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1933 - accuracy: 0.9586\n",
      "Epoch 659/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2205 - accuracy: 0.9533\n",
      "Epoch 660/700\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2348 - accuracy: 0.9509\n",
      "Epoch 661/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2086 - accuracy: 0.9563\n",
      "Epoch 662/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2478 - accuracy: 0.9504\n",
      "Epoch 663/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1950 - accuracy: 0.9593\n",
      "Epoch 664/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1991 - accuracy: 0.9584\n",
      "Epoch 665/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1838 - accuracy: 0.9613\n",
      "Epoch 666/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1748 - accuracy: 0.9609\n",
      "Epoch 667/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1900 - accuracy: 0.9592\n",
      "Epoch 668/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1877 - accuracy: 0.9600\n",
      "Epoch 669/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2098 - accuracy: 0.9568\n",
      "Epoch 670/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2506 - accuracy: 0.9496\n",
      "Epoch 671/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2606 - accuracy: 0.9493\n",
      "Epoch 672/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2454 - accuracy: 0.9503\n",
      "Epoch 673/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2467 - accuracy: 0.9493\n",
      "Epoch 674/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2276 - accuracy: 0.9537\n",
      "Epoch 675/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2225 - accuracy: 0.9534\n",
      "Epoch 676/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1954 - accuracy: 0.9572\n",
      "Epoch 677/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1796 - accuracy: 0.9606\n",
      "Epoch 678/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1893 - accuracy: 0.9603\n",
      "Epoch 679/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1783 - accuracy: 0.9614\n",
      "Epoch 680/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1908 - accuracy: 0.9611\n",
      "Epoch 681/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2361 - accuracy: 0.9527\n",
      "Epoch 682/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1570 - accuracy: 0.9655\n",
      "Epoch 683/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1863 - accuracy: 0.9611\n",
      "Epoch 684/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.1677 - accuracy: 0.9634\n",
      "Epoch 685/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2208 - accuracy: 0.9564\n",
      "Epoch 686/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2126 - accuracy: 0.9565\n",
      "Epoch 687/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2294 - accuracy: 0.9548\n",
      "Epoch 688/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1947 - accuracy: 0.9598\n",
      "Epoch 689/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2422 - accuracy: 0.9516\n",
      "Epoch 690/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1844 - accuracy: 0.9609\n",
      "Epoch 691/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1617 - accuracy: 0.9640\n",
      "Epoch 692/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1780 - accuracy: 0.9623\n",
      "Epoch 693/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1997 - accuracy: 0.9591\n",
      "Epoch 694/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1663 - accuracy: 0.9641\n",
      "Epoch 695/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.2401 - accuracy: 0.9549\n",
      "Epoch 696/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.2395 - accuracy: 0.9521\n",
      "Epoch 697/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2100 - accuracy: 0.9567\n",
      "Epoch 698/700\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.2013 - accuracy: 0.9582\n",
      "Epoch 699/700\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2096 - accuracy: 0.9566\n",
      "Epoch 700/700\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2056 - accuracy: 0.9577\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 9.2368 - accuracy: 0.5787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.236783981323242, 0.5787000060081482]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model=cnn_model()\n",
    "cnn_model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "cnn_model.summary()\n",
    "cnn_model.fit(x_train, y_train, epochs=700, shuffle=True)\n",
    "cnn_model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CF3J53e3nr9M",
   "metadata": {
    "id": "CF3J53e3nr9M"
   },
   "source": [
    "### Results of ReNet-5 with batch size=1600,epochs=500,adam optimizer,training 4 hours 3 minutes,loss=categorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hkIg0FVkn7Qb",
   "metadata": {
    "id": "hkIg0FVkn7Qb"
   },
   "source": [
    "Train loss: 0.0062 - accuracy: 1.0000\\\n",
    "Test loss: 5.2187 - accuracy: 0.6113"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hRnHbq0-lbpc",
   "metadata": {
    "id": "hRnHbq0-lbpc"
   },
   "source": [
    "### Results of ReNet-5 with batch size=400,epochs=500,sgd optimizer,training some hours,loss=categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WzTQvwjZlcf5",
   "metadata": {
    "id": "WzTQvwjZlcf5"
   },
   "source": [
    "Train loss: 0.2771 - accuracy: 0.9027\\\n",
    "Test loss: 1.7955 - accuracy: 0.6421"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uftLF2w9qCb0",
   "metadata": {
    "id": "uftLF2w9qCb0"
   },
   "source": [
    "### Evaluation:\n",
    "Test losses are high in every ocassion.Adam optimizer has higher train accuracy and lower train loss  but sgd optimizer higher test accuracy and lower test loss ,with more epochs sgd will be better.Is better to have batches size instead of None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcffaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some useful links:\n",
    "\n",
    "## https://www.datacamp.com/tutorial/k-nearest-neighbor-classification-scikit-learn\n",
    "## https://www.edureka.co/blog/k-nearest-neighbors-algorithm/\n",
    "## https://www.datatechnotes.com/2020/08/classification-with-nearest-centroid-in-python.html\n",
    "## https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-6-68\n",
    "## https://www.geeksforgeeks.org/ml-nearest-centroid-classifier/\n",
    "## https://keras.io/guides/preprocessing_layers/\n",
    "## https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "## https://www.geeksforgeeks.org/cross-entropy-cost-functions-used-in-classification/\n",
    "## https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/\n",
    "## https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1\n",
    "## https://www.youtube.com/watch?v=zwSXSltRhh0\n",
    "## https://www.geeksforgeeks.org/principal-component-analysis-with-python/\n",
    "## https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YAkrMeLWzHy7",
    "0mWm0CUSKfA9",
    "XInPGvtzetsr",
    "bPmoADhn8e_o",
    "CF3J53e3nr9M",
    "hRnHbq0-lbpc"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
